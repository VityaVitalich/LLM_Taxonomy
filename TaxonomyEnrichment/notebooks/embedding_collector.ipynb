{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "SAVING_DIR = '/home/data/taxonomy/'\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = SAVING_DIR + \"hf_cache/\"\n",
    "os.environ[\"HF_HOME\"] = SAVING_DIR + \"hf_cache/\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import pickle\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaModel\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_edgelist('../data/MAG_CS/all.edgelist', create_using=nx.DiGraph, delimiter='\\t')\n",
    "nodes = list(G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = []\n",
    "for node in nodes:\n",
    "    elem = {}\n",
    "    elem[\"children\"] = node\n",
    "    elem[\"parents\"] = ''\n",
    "    elem[\"grandparents\"] = None\n",
    "    elem[\"case\"] = \"predict_hypernym\"\n",
    "    all_nodes.append(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29484\n"
     ]
    }
   ],
   "source": [
    "print(len(all_nodes))\n",
    "with open('../data/MAG_CS/all_nodes.pickle', 'wb') as f:\n",
    "    pickle.dump(all_nodes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d997c0409de04b2384060b7a7f7435f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1727: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "HF_TOKEN = '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'\n",
    "model_type = LlamaForCausalLM\n",
    "tokenizer_type = LlamaTokenizer\n",
    "model_checkpoint = 'meta-llama/Llama-2-7b-hf'\n",
    "\n",
    "extra_model_params = {}\n",
    "extra_model_params[\"torch_dtype\"] = torch.bfloat16\n",
    "extra_model_params[\"load_in_4bit\"] = True\n",
    "\n",
    "model = model_type.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    device_map=\"auto\",\n",
    "    use_auth_token=HF_TOKEN,\n",
    "    **extra_model_params\n",
    ").eval()\n",
    "\n",
    "tokenizer = tokenizer_type.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    padding_side=\"left\",\n",
    "    use_auth_token=HF_TOKEN,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    }
   ],
   "source": [
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q\",\n",
    "    \"v\",\n",
    "]\n",
    "\n",
    "# model = prepare_model_for_int8_training(model)\n",
    "config_lora = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    # target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config_lora)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = ['hello']\n",
    "tokens = tokenizer.batch_encode_plus(batch, return_tensors='pt', padding=True).to(device)\n",
    "out = model.base_model.model.model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 4096])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['last_hidden_state'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = '/home/data/taxonomy/model_checkpoints/meta-llama-Llama-2-7b-hfonly-wnet-57seed-32bs_epoch=0_MAP=0.23933677520010854.pth'\n",
    "\n",
    "checkpoint = torch.load(load_path, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ckpt = {}\n",
    "for k, v in checkpoint['model'].items():\n",
    "    new_name = k.replace('base_model.model', 'base_model')\n",
    "    new_ckpt[new_name] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22544384, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['model']['base_model.model.model.layers.0.mlp.down_proj.weight'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'base_model.model.embed_tokens.weight'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'base_model.model.model.embed_tokens.weight'\n",
    "s.replace('base_model.model', 'base_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PeftModelForCausalLM:\n\tUnexpected key(s) in state_dict: \"base_model.lm_head.weight\". \n\tsize mismatch for base_model.model.layers.0.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.0.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.0.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.0.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.0.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.0.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.0.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.1.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.1.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.1.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.1.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.1.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.1.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.1.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.2.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.2.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.2.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.2.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.2.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.2.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.2.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.3.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.3.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.3.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.3.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.3.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.3.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.3.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.4.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.4.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.4.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.4.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.4.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.4.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.4.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.5.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.5.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.5.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.5.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.5.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.5.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.5.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.6.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.6.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.6.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.6.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.6.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.6.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.6.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.7.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.7.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.7.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.7.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.7.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.7.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.7.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.8.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.8.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.8.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.8.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.8.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.8.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.8.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.9.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.9.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.9.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.9.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.9.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.9.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.9.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.10.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.10.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.10.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.10.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.10.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.10.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.10.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.11.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.11.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.11.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.11.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.11.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.11.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.11.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.12.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.12.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.12.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.12.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.12.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.12.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.12.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.13.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.13.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.13.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.13.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.13.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.13.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.13.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.14.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.14.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.14.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.14.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.14.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.14.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.14.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.15.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.15.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.15.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.15.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.15.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.15.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.15.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.16.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.16.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.16.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.16.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.16.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.16.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.16.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.17.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.17.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.17.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.17.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.17.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.17.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.17.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.18.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.18.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.18.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.18.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.18.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.18.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.18.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.19.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.19.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.19.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.19.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.19.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.19.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.19.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.20.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.20.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.20.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.20.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.20.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.20.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.20.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.21.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.21.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.21.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.21.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.21.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.21.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.21.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.22.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.22.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.22.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.22.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.22.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.22.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.22.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.23.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.23.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.23.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.23.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.23.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.23.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.23.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.24.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.24.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.24.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.24.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.24.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.24.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.24.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.25.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.25.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.25.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.25.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.25.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.25.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.25.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.26.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.26.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.26.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.26.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.26.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.26.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.26.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.27.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.27.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.27.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.27.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.27.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.27.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.27.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.28.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.28.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.28.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.28.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.28.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.28.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.28.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.29.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.29.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.29.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.29.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.29.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.29.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.29.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.30.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.30.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.30.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.30.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.30.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.30.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.30.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.31.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.31.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.31.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.31.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.31.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.31.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.31.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_ckpt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m checkpoint\n\u001b[1;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PeftModelForCausalLM:\n\tUnexpected key(s) in state_dict: \"base_model.lm_head.weight\". \n\tsize mismatch for base_model.model.layers.0.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.0.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.0.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.0.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.0.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.0.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.0.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.1.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.1.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.1.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.1.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.1.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.1.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.1.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.2.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.2.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.2.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.2.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.2.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.2.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.2.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.3.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.3.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.3.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.3.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.3.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.3.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.3.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.4.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.4.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.4.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.4.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.4.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.4.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.4.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.5.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.5.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.5.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.5.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.5.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.5.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.5.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.6.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.6.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.6.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.6.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.6.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.6.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.6.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.7.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.7.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.7.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.7.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.7.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.7.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.7.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.8.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.8.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.8.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.8.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.8.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.8.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.8.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.9.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.9.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.9.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.9.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.9.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.9.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.9.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.10.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.10.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.10.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.10.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.10.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.10.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.10.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.11.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.11.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.11.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.11.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.11.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.11.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.11.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.12.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.12.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.12.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.12.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.12.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.12.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.12.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.13.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.13.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.13.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.13.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.13.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.13.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.13.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.14.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.14.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.14.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.14.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.14.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.14.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.14.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.15.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.15.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.15.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.15.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.15.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.15.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.15.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.16.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.16.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.16.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.16.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.16.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.16.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.16.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.17.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.17.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.17.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.17.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.17.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.17.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.17.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.18.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.18.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.18.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.18.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.18.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.18.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.18.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.19.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.19.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.19.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.19.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.19.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.19.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.19.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.20.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.20.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.20.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.20.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.20.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.20.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.20.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.21.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.21.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.21.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.21.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.21.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.21.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.21.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.22.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.22.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.22.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.22.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.22.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.22.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.22.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.23.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.23.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.23.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.23.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.23.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.23.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.23.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.24.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.24.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.24.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.24.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.24.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.24.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.24.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.25.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.25.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.25.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.25.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.25.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.25.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.25.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.26.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.26.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.26.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.26.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.26.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.26.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.26.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.27.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.27.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.27.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.27.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.27.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.27.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.27.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.28.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.28.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.28.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.28.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.28.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.28.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.28.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.29.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.29.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.29.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.29.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.29.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.29.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.29.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.30.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.30.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.30.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.30.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.30.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.30.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.30.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for base_model.model.layers.31.self_attn.q_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.31.self_attn.k_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.31.self_attn.v_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.31.self_attn.o_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for base_model.model.layers.31.mlp.gate_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.31.mlp.up_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for base_model.model.layers.31.mlp.down_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008])."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(new_ckpt)\n",
    "del checkpoint\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "def divide_chunks(l, n): \n",
    "    \n",
    "    # looping till length l \n",
    "    for i in range(0, len(l), n):  \n",
    "        yield l[i:i + n] \n",
    "\n",
    "G = nx.read_edgelist('../data/MAG_CS/all.edgelist', create_using=nx.DiGraph, delimiter='\\t')\n",
    "\n",
    "nodes = list(G.nodes)\n",
    "bs = 32\n",
    "\n",
    "all_embeddings = {}\n",
    "\n",
    "\n",
    "for batch in tqdm(divide_chunks(nodes, bs)):\n",
    "    tokens = tokenizer.batch_encode_plus(batch, return_tensors='pt', padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**tokens, output_hidden_states=True)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 4096])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['last_hidden_state'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = tokens['attention_mask'].sum(dim=1)\n",
    "out['last_hidden_state'][:, seq_len-1, :].diagonal().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "ft = fasttext.load_model('/home/data/taxonomy/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29484 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29484/29484 [00:00<00:00, 63141.06it/s]\n"
     ]
    }
   ],
   "source": [
    "nodes = list(G.nodes)\n",
    "all_embeddings = {}\n",
    "        \n",
    "for word in tqdm(nodes):\n",
    "    all_embeddings[word] = ft.get_sentence_vector(word)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(all_embeddings.keys()) == len(G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/MAG_CS/embeddings_ft_sentence.pickle', 'wb') as f:\n",
    "    pickle.dump(all_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85956f3514e04b768dfa7d7f0cbcb2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c0301b31854ff39e03386cf7c82e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39dfb7eac9ae41238be1d673c8316faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd6fb1b0265485f8c915f7a1886e261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(G.nodes)\n",
    "bs = 32\n",
    "\n",
    "all_embeddings = {}\n",
    "\n",
    "def divide_chunks(l, n): \n",
    "    \n",
    "    # looping till length l \n",
    "    for i in range(0, len(l), n):  \n",
    "        yield l[i:i + n] \n",
    "\n",
    "for batch in tqdm(divide_chunks(nodes, bs)):\n",
    "    tokens = tokenizer.batch_encode_plus(batch, return_tensors='pt', padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m \u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "seq_len = tokens['attention_mask'].sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 9, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['last_hidden_state'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/formatters.py:708\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    701\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    702\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    704\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    705\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    706\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    707\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 708\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:426\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    423\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    424\u001b[0m     )\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor_str.py:636\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    635\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor_str.py:567\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    565\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    566\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    570\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor_str.py:327\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    325\u001b[0m     )\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor_str.py:111\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfloating_dtype:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m tensor_view:\n\u001b[0;32m--> 111\u001b[0m         value_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:872\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[0;32m--> 872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "out['last_hidden_state'][seq_len.repeat(32, 1, 768) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_outputs = (out['last_hidden_state'] * tokens['attention_mask'].unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = (masked_outputs.sum(dim=-2) / tokens['attention_mask'].sum(dim=1).unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05684492, -0.4693739 ,  0.19657701, ...,  0.06092722,\n",
       "        -0.12598057, -0.14159232],\n",
       "       [-0.00673314, -0.32868305,  0.11917636, ..., -0.1555016 ,\n",
       "        -0.16298126, -0.02167546],\n",
       "       [-0.2480003 , -0.5751185 ,  0.14767723, ...,  0.08540212,\n",
       "        -0.09679956, -0.12009169],\n",
       "       ...,\n",
       "       [-0.07648315, -0.24393408, -0.1527339 , ..., -0.27309507,\n",
       "        -0.1798447 ,  0.10657488],\n",
       "       [ 0.19035351, -0.00463882,  0.11054821, ...,  0.03654246,\n",
       "         0.0472547 , -0.03532267],\n",
       "       [-0.13807954, -0.24475516, -0.16560027, ..., -0.17918664,\n",
       "        -0.00962719, -0.07841179]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'big bang nucleosynthesis'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 10147, 12079,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  2152,  3177,  7642,  4957,   102,     0,     0,     0],\n",
       "        [  101, 15453,  5657,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  2951,  7790, 10147, 12079,   102,     0,     0,     0],\n",
       "        [  101, 23760, 13767,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  4874, 23760, 13767,  2075,   102,     0,     0,     0],\n",
       "        [  101, 23760, 18209,  9260,  2291,   102,     0,     0,     0],\n",
       "        [  101,  5009,  2000,  2592,  4216,   102,     0,     0,     0],\n",
       "        [  101, 15373,   102,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  3609,  5024,   102,     0,     0,     0,     0,     0],\n",
       "        [  101,  4744,  3409,  2121,   102,     0,     0,     0,     0],\n",
       "        [  101,  7524,  2707,  2051,   102,     0,     0,     0,     0],\n",
       "        [  101,  7524,  2203,  2051,   102,     0,     0,     0,     0],\n",
       "        [  101, 16371, 14321,  2891,  6038, 25078,   102,     0,     0],\n",
       "        [  101,  1038,  2475,  2546,  2232,  3259,   102,     0,     0],\n",
       "        [  101,  2502,  9748, 16371, 14321,  2891,  6038, 25078,   102]],\n",
       "       device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
