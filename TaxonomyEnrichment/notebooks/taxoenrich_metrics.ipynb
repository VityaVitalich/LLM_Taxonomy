{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange(energy_scores, candidate_position_idx, true_position_idx):\n",
    "    tmp = np.array([[x==y for x in candidate_position_idx] for y in true_position_idx]).any(0)\n",
    "    correct = np.where(tmp)[0]\n",
    "    incorrect = np.where(~tmp)[0]\n",
    "    labels = torch.cat((torch.ones(len(correct)), torch.zeros(len(incorrect)))).int()\n",
    "    energy_scores = torch.cat((energy_scores[correct], energy_scores[incorrect]))\n",
    "    return energy_scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def calculate_ranks_from_distance(all_distances, positive_relations):\n",
    "    \"\"\"\n",
    "    all_distances: a np array\n",
    "    positive_relations: a list of array indices\n",
    "\n",
    "    return a list\n",
    "    \"\"\"\n",
    "    # positive_relation_distance = all_distances[positive_relations]\n",
    "    # negative_relation_distance = np.ma.array(all_distances, mask=False)\n",
    "    # negative_relation_distance.mask[positive_relations] = True\n",
    "    # ranks = list((negative_relation_distance < positive_relation_distance[:, np.newaxis]).sum(axis=1) + 1)\n",
    "    # ranks = list((all_distances < positive_relation_distance[:, np.newaxis]).sum(axis=1) + 1)\n",
    "    ranks = list(np.argsort(np.argsort(all_distances))[positive_relations]+1)\n",
    "    return ranks\n",
    "\n",
    "def obtain_ranks(outputs, targets):\n",
    "    \"\"\" \n",
    "    outputs : tensor of size (batch_size, 1), required_grad = False, model predictions\n",
    "    targets : tensor of size (batch_size, ), required_grad = False, labels\n",
    "        Assume to be of format [1, 0, ..., 0, 1, 0, ..., 0, ..., 0]\n",
    "    mode == 0: rank from distance (smaller is preferred)\n",
    "    mode == 1: rank from similarity (larger is preferred)\n",
    "    \"\"\"\n",
    "    calculate_ranks = calculate_ranks_from_distance\n",
    "    all_ranks = []\n",
    "    prediction = outputs.cpu().numpy().squeeze()\n",
    "    label = targets.cpu().numpy()\n",
    "    sep = np.array([0, 1], dtype=label.dtype)\n",
    "    \n",
    "    # fast way to find subarray indices in a large array, c.f. https://stackoverflow.com/questions/14890216/return-the-indexes-of-a-sub-array-in-an-array\n",
    "    end_indices = [(m.start() // label.itemsize)+1 for m in re.finditer(sep.tostring(), label.tostring())]\n",
    "    end_indices.append(len(label)+1)\n",
    "    start_indices = [0] + end_indices[:-1]\n",
    "    for start_idx, end_idx in zip(start_indices, end_indices):\n",
    "        distances = prediction[start_idx: end_idx]\n",
    "        labels = label[start_idx:end_idx]\n",
    "        positive_relations = list(np.where(labels == 1)[0])\n",
    "        ranks = calculate_ranks(distances, positive_relations)\n",
    "        all_ranks.append(ranks)\n",
    "    return all_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def macro_mr(all_ranks):\n",
    "    macro_mr = np.array([np.array(all_rank).mean() for all_rank in all_ranks]).mean()\n",
    "    return macro_mr\n",
    "\n",
    "def micro_mr(all_ranks):\n",
    "    micro_mr = np.array(list(itertools.chain(*all_ranks))).mean()\n",
    "    return micro_mr\n",
    "\n",
    "def hit_at_1(all_ranks):\n",
    "    rank_positions = np.array(list(itertools.chain(*all_ranks)))\n",
    "    hits = np.sum(rank_positions <= 1)\n",
    "    return 1.0 * hits / len(rank_positions)\n",
    "\n",
    "def hit_at_3(all_ranks):\n",
    "    rank_positions = np.array(list(itertools.chain(*all_ranks)))\n",
    "    hits = np.sum(rank_positions <= 3)\n",
    "    return 1.0 * hits / len(rank_positions)\n",
    "\n",
    "def hit_at_5(all_ranks):\n",
    "    rank_positions = np.array(list(itertools.chain(*all_ranks)))\n",
    "    hits = np.sum(rank_positions <= 5)\n",
    "    return 1.0 * hits / len(rank_positions)\n",
    "\n",
    "def hit_at_10(all_ranks):\n",
    "    rank_positions = np.array(list(itertools.chain(*all_ranks)))\n",
    "    hits = np.sum(rank_positions <= 10)\n",
    "    return 1.0 * hits / len(rank_positions)\n",
    "\n",
    "def precision_at_1(all_ranks):\n",
    "    rank_positions = np.array(list(itertools.chain(*all_ranks)))\n",
    "    hits = np.sum(rank_positions <= 1)\n",
    "    return 1.0 * hits / len(all_ranks)\n",
    "\n",
    "def precision_at_3(all_ranks):\n",
    "    rank_positions = np.array(list(itertools.chain(*all_ranks)))\n",
    "    hits = np.sum(rank_positions <= 3)\n",
    "    return 1.0 * hits / (len(all_ranks)*3)\n",
    "\n",
    "def precision_at_5(all_ranks):\n",
    "    rank_positions = np.array(list(itertools.chain(*all_ranks)))\n",
    "    hits = np.sum(rank_positions <= 5)\n",
    "    return 1.0 * hits / (len(all_ranks)*5)\n",
    "\n",
    "def precision_at_10(all_ranks):\n",
    "    rank_positions = np.array(list(itertools.chain(*all_ranks)))\n",
    "    hits = np.sum(rank_positions <= 10)\n",
    "    return 1.0 * hits / (len(all_ranks)*10)\n",
    "\n",
    "def mrr_scaled_10(all_ranks):\n",
    "    \"\"\" Scaled MRR score, check eq. (2) in the PinSAGE paper: https://arxiv.org/pdf/1806.01973.pdf\n",
    "    \"\"\"\n",
    "    rank_positions = np.array(list(itertools.chain(*all_ranks)))\n",
    "    \n",
    "    scaled_rank_positions = np.ceil(rank_positions / 10)\n",
    " #   print(scaled_rank_positions, (1.0 / scaled_rank_positions).mean())\n",
    "    return (1.0 / scaled_rank_positions).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#test_path = '../data/psychology/test_nodes.pickle'\n",
    "test_path = '../data/noun/test_hypernyms_def.pickle'\n",
    "with open(test_path, 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "test_wn = []\n",
    "\n",
    "for elem in test:\n",
    "    case = elem['case']\n",
    "    if case == 'predict_hypernym':\n",
    "        cur_pair = (elem['children'], [elem['parents']])\n",
    "    else:\n",
    "        cur_pair = (elem['children'], elem['parents'])\n",
    "\n",
    "    test_wn.append(cur_pair)\n",
    "test = test_wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('unconventionality.n.02', ['unorthodoxy.n.03'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    return s.split('.')[0]\n",
    "\n",
    "new_test = []\n",
    "for child, parents in test:\n",
    "    temp = []\n",
    "    for parent in parents:\n",
    "        temp.append((clean(parent), clean(child)))\n",
    "    new_test.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = '../../../data/taxonomy/model_outputs/_meta-llama-Llama-2-7b-hfTaxoEnrich_noun_Unified_3beams_40topk_0.8temp_3norepeat_stohastic_'\n",
    "with open(pred_path, 'rb') as f:\n",
    "    pred = pickle.load(f)\n",
    "\n",
    "def get_hypernyms(line):\n",
    "    clean_line = line.strip().replace(\"\\n\", \",\").split(\",\")\n",
    "\n",
    "    res = []\n",
    "    for hyp in clean_line:\n",
    "        if not hyp in (\"\", \" \", \", \", \",\"):\n",
    "            res.append(hyp.lower().strip())\n",
    "\n",
    "    return res\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def unique_words_by_frequency(words):\n",
    "    # Count the frequency of each word in the list\n",
    "    frequency = Counter(words)\n",
    "    # Sort the words first by frequency, then by the order they appear in the original list\n",
    "    sorted_words = sorted(set(words), key=lambda x: (-frequency[x], words.index(x)))\n",
    "    return sorted_words\n",
    "\n",
    "concat = True\n",
    "\n",
    "new_pred = []\n",
    "for elem in pred:\n",
    "    if concat:\n",
    "        cur_portion = []\n",
    "        for line in elem:\n",
    "            cur_portion.extend(get_hypernyms(line))\n",
    "        new_pred.append(unique_words_by_frequency(cur_portion))\n",
    "    else:\n",
    "        new_pred.append(get_hypernyms(elem[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[('unorthodoxy', 'unconventionality')],\n",
       "  [('brome', 'awnless_bromegrass')],\n",
       "  [('insufficiency', 'slenderness')]],\n",
       " [['unorthodoxiness',\n",
       "   'unorthodxy',\n",
       "   'unconformity',\n",
       "   'heterodoxy',\n",
       "   'dissidence',\n",
       "   'disobedience',\n",
       "   'disloyalty',\n",
       "   'dis',\n",
       "   'disaffection',\n",
       "   'dissent',\n",
       "   'disagreement'],\n",
       "  ['bromus',\n",
       "   'brome',\n",
       "   'bromine grass',\n",
       "   'foxtail',\n",
       "   \"fox's tail\",\n",
       "   'caryopsis',\n",
       "   'cereal grass',\n",
       "   'c',\n",
       "   'briza',\n",
       "   'breeze grass',\n",
       "   'b'],\n",
       "  ['weakness',\n",
       "   'insufficiency',\n",
       "   'deficiency',\n",
       "   'shortcoming',\n",
       "   'incompleteness',\n",
       "   'imperfection',\n",
       "   'indefiniteness',\n",
       "   'insubstantial',\n",
       "   'inadequacies']])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test[:3], new_pred[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_67082/4141578937.py:33: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  end_indices = [(m.start() // label.itemsize)+1 for m in re.finditer(sep.tostring(), label.tostring())]\n",
      "/tmp/ipykernel_67082/1455042432.py:58: RuntimeWarning: Mean of empty slice.\n",
      "  return (1.0 / scaled_rank_positions).mean()\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/tmp/ipykernel_67082/1455042432.py:14: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return 1.0 * hits / len(rank_positions)\n",
      "/tmp/ipykernel_67082/1455042432.py:24: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return 1.0 * hits / len(rank_positions)\n"
     ]
    }
   ],
   "source": [
    "metric_names = {\n",
    "    'mrr': mrr_scaled_10,\n",
    "    'p1': precision_at_1,\n",
    "    'p5': precision_at_5,\n",
    "    'r1': hit_at_1,\n",
    "    'r5': hit_at_5\n",
    "}\n",
    "\n",
    "metrics = {}\n",
    "for name in metric_names.keys():\n",
    "    metrics[name] = []\n",
    "for idx in range(len(new_test)):\n",
    "    #hyps = get_hypernyms(new_pred[idx])\n",
    "    hyps = new_pred[idx] + [', ']\n",
    "   # print(hyps)\n",
    "    gold = new_test[idx]\n",
    "\n",
    "    child = gold[0][1]\n",
    "    new_hyps = [(hyp, child) for hyp in hyps]\n",
    "    scores = torch.arange(len(new_hyps))\n",
    "\n",
    "    batched_energy_scores, labels = rearrange(scores, new_hyps, gold)\n",
    "\n",
    "    all_ranks = obtain_ranks(batched_energy_scores, labels)\n",
    "    for name, func in metric_names.items():\n",
    "        cur_metric = np.nan_to_num(func(all_ranks))\n",
    "        metrics[name].append(cur_metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrr 0.48020304568527916\n",
      "p1 0.38274111675126904\n",
      "p5 0.09401015228426396\n",
      "r1 0.3822335025380711\n",
      "r5 0.46954314720812185\n"
     ]
    }
   ],
   "source": [
    "for name, v in metrics.items():\n",
    "    print(name, np.mean(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "985"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(metrics['mrr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "473"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(metrics['mrr']) > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
