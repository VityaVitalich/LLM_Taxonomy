{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "sys.path.append('../../')\n",
    "from DataConstructor.notebooks.leafer import Leafer\n",
    "\n",
    "\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_edgelist(\"../../TaxonomyEnrichment/data/MAG_CS/train.edgelist\", delimiter=\"\\t\", create_using=nx.DiGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lock', 'lock')]\n",
      "[('memory management', 'flat memory model'), ('flat memory model', 'virtual memory'), ('virtual memory', 'memory management')]\n",
      "[('memory management', 'memory map'), ('memory map', 'virtual memory'), ('virtual memory', 'memory management')]\n",
      "[('obfuscation', 'obfuscation')]\n",
      "[('tautology', 'tautology')]\n",
      "[('aliasing', 'aliasing')]\n",
      "[('change management', 'change management')]\n",
      "[('intelligence cycle', 'intelligence cycle')]\n",
      "[('fragmentation', 'fragmentation')]\n",
      "[('kernel', 'kernel')]\n",
      "[('kernel', 'kernel method'), ('kernel method', 'radial basis function kernel'), ('radial basis function kernel', 'kernel')]\n",
      "[('kernel', 'radial basis function kernel'), ('radial basis function kernel', 'kernel')]\n",
      "[('kernel', 'kernel embedding of distributions'), ('kernel embedding of distributions', 'kernel')]\n",
      "[('polynomial kernel', 'kernel'), ('kernel', 'polynomial kernel')]\n",
      "[('kernel', 'kernel principal component analysis'), ('kernel principal component analysis', 'kernel')]\n",
      "[('critical mass', 'critical mass')]\n",
      "[('quadratic classifier', 'margin'), ('margin', 'classifier'), ('classifier', 'quadratic classifier')]\n",
      "[('margin classifier', 'margin'), ('margin', 'classifier'), ('classifier', 'margin classifier')]\n",
      "[('r tree', 'r tree')]\n",
      "[('level set', 'signed distance function'), ('signed distance function', 'level set')]\n",
      "[('level set method', 'level set'), ('level set', 'level set method')]\n",
      "[('pattern recognition', 'pattern recognition')]\n",
      "[('channel', 'channel')]\n",
      "[('channel', 'telecommunications link'), ('telecommunications link', 'control channel'), ('control channel', 'channel')]\n",
      "[('aspect ratio', 'aspect ratio')]\n",
      "[('data synchronization', 'synchronization'), ('synchronization', 'data synchronization')]\n",
      "[('bootstrapping', 'bootstrapping')]\n",
      "[('syntax', 'abstract syntax'), ('abstract syntax', 'syntax')]\n",
      "[('control zone', 'airspace class'), ('airspace class', 'control zone')]\n",
      "[('airspace class', 'controlled airspace'), ('controlled airspace', 'airspace class')]\n",
      "[('airspace class', 'national airspace system'), ('national airspace system', 'airspace class')]\n",
      "[('triple modular redundancy', 'redundancy'), ('redundancy', 'triple modular redundancy')]\n",
      "[('middleware', 'middleware')]\n",
      "[('locale', 'locale')]\n",
      "[('formal semantics', 'well founded semantics'), ('well founded semantics', 'semantics'), ('semantics', 'formal semantics')]\n",
      "[('formal semantics', 'formal semantics')]\n",
      "[('formal semantics', 'computational semantics'), ('computational semantics', 'semantics'), ('semantics', 'formal semantics')]\n",
      "[('well founded semantics', 'semantics'), ('semantics', 'operational semantics'), ('operational semantics', 'well founded semantics')]\n",
      "[('computational semantics', 'semantics'), ('semantics', 'operational semantics'), ('operational semantics', 'computational semantics')]\n",
      "[('normalization', 'normalization')]\n",
      "[('interpolation', 'bilinear interpolation'), ('bilinear interpolation', 'image scaling'), ('image scaling', 'interpolation')]\n",
      "[('feature', 'feature')]\n",
      "[('pointer', 'pointer')]\n",
      "[('bowyer watson algorithm', 'minimum weight triangulation'), ('minimum weight triangulation', 'triangulation'), ('triangulation', 'bowyer watson algorithm')]\n",
      "[('bowyer watson algorithm', 'surface triangulation'), ('surface triangulation', 'triangulation'), ('triangulation', 'bowyer watson algorithm')]\n",
      "[('constrained delaunay triangulation', 'minimum weight triangulation'), ('minimum weight triangulation', 'triangulation'), ('triangulation', 'constrained delaunay triangulation')]\n",
      "[('constrained delaunay triangulation', 'surface triangulation'), ('surface triangulation', 'triangulation'), ('triangulation', 'constrained delaunay triangulation')]\n",
      "[('triangulation', 'triangulation')]\n",
      "[('pitteway triangulation', 'minimum weight triangulation'), ('minimum weight triangulation', 'triangulation'), ('triangulation', 'pitteway triangulation')]\n",
      "[('degrees of freedom', 'degrees of freedom')]\n",
      "[('filter', 'filter')]\n",
      "[('semantics', 'operational semantics'), ('operational semantics', 'denotational semantics'), ('denotational semantics', 'semantics')]\n",
      "[('heuristic', 'heuristic')]\n",
      "[('chunking', 'chunking')]\n",
      "[('panning', 'panning')]\n",
      "[('transponder', 'transponder')]\n",
      "[('modularity', 'modularity')]\n",
      "[('authentication', 'authentication')]\n",
      "[('resource allocation', 'resource allocation')]\n",
      "[('throughput', 'throughput')]\n",
      "[('modes of convergence', 'normal convergence'), ('normal convergence', 'modes of convergence')]\n",
      "[('sandbox', 'sandbox')]\n",
      "[('beam tracing', 'ray tracing'), ('ray tracing', 'distributed ray tracing'), ('distributed ray tracing', 'beam tracing')]\n",
      "[('ray tracing', 'distributed ray tracing'), ('distributed ray tracing', 'cone tracing'), ('cone tracing', 'ray tracing')]\n",
      "[('ontology', 'ontology')]\n",
      "[('recursion', 'recursion')]\n",
      "[('hop', 'hop')]\n",
      "[('node', 'ring network'), ('ring network', 'node')]\n",
      "[('broadcasting', 'broadcasting')]\n",
      "[('modulation', 'modulation')]\n",
      "[('distortion', 'distortion')]\n",
      "[('conceptual model', 'conceptual model')]\n",
      "[('configuration management', 'configuration management')]\n",
      "[('conjoint analysis', 'conjoint analysis')]\n",
      "[('operator', 'operator')]\n",
      "[('stochastic resonance', 'stochastic resonance')]\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        cycle = nx.find_cycle(G)\n",
    "        print(cycle)\n",
    "        G.remove_edge(*cycle[0])\n",
    "    except:\n",
    "        break\n",
    "\n",
    "new_labels = {}\n",
    "for node in G.nodes():\n",
    "    new_labels[node] = node + '.n.1'\n",
    "\n",
    "G_new = nx.relabel_nodes(G, new_labels)\n",
    "\n",
    "l = Leafer(G_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_hypernym 24706 24706\n"
     ]
    }
   ],
   "source": [
    "train, test = l.split_train_test(\n",
    "    generation_depth=0,  # до какого уровня в топ. сортировке идти\n",
    "    p=0.0,  # вероятность что подходящий случай уйдет в тест\n",
    "    p_divide_leafs=0.5,\n",
    "    # вероятность что листья поделим пополам трейн-тест\n",
    "    # а не засунем целый случай в трейн или в тест\n",
    "    min_to_test_rate=0.5,\n",
    "    # минимальное количество доли вершин которых не было в\n",
    "    # трейне чтобы поделить пополам на трейн-тест\n",
    "    # то есть если 6\\10 вершин были трейне то значит все 10 в трейн\n",
    "    # если 5\\10 были в трейне, то значит оставшиеся можем кинуть в тест\n",
    "    weights=[0.00, 0.0, 0.0, 0.00, 0.00, 1.],\n",
    "    # веса в соответствии\n",
    "    # один ребенок, только листья, не только листья\n",
    "    # триплеты с 2 родителями, триплеты такие что мидл нода имеет\n",
    "    # 1 ребенка, предсказание родителя\n",
    "    #p_parent=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '../data/MAG_CS/test_nodes.pickle'\n",
    "with open(test_path, 'rb') as f:\n",
    "    test_nodes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spanish verbs', ['verb']),\n",
       " ('effective transmission rate', ['wireless', 'channel']),\n",
       " ('achterbahn', ['stream cipher', 'cryptanalysis']),\n",
       " ('yukagir language', ['verb']),\n",
       " ('toroidal coordinates',\n",
       "  ['elliptic coordinate system', 'parabolic coordinates'])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nodes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "\n",
    "for child, parents in test_nodes:\n",
    "    elem = {}\n",
    "    elem[\"children\"] = child\n",
    "    elem[\"grandparents\"] = None\n",
    "    if len(parents) == 1:\n",
    "        elem[\"parents\"] = parents[0]\n",
    "        elem[\"case\"] = \"predict_hypernym\"\n",
    "    else:\n",
    "        elem[\"parents\"] = parents\n",
    "        elem[\"case\"] = \"predict_multiple_hypernyms\"\n",
    "\n",
    "    test.append(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'children': 'spanish verbs',\n",
       "  'grandparents': None,\n",
       "  'parents': 'verb',\n",
       "  'case': 'predict_hypernym'},\n",
       " {'children': 'effective transmission rate',\n",
       "  'grandparents': None,\n",
       "  'parents': ['wireless', 'channel'],\n",
       "  'case': 'predict_multiple_hypernyms'},\n",
       " {'children': 'achterbahn',\n",
       "  'grandparents': None,\n",
       "  'parents': ['stream cipher', 'cryptanalysis'],\n",
       "  'case': 'predict_multiple_hypernyms'},\n",
       " {'children': 'yukagir language',\n",
       "  'grandparents': None,\n",
       "  'parents': 'verb',\n",
       "  'case': 'predict_hypernym'},\n",
       " {'children': 'toroidal coordinates',\n",
       "  'grandparents': None,\n",
       "  'parents': ['elliptic coordinate system', 'parabolic coordinates'],\n",
       "  'case': 'predict_multiple_hypernyms'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'children': 'cabinet card.n.1',\n",
       "  'parents': 'studio.n.1',\n",
       "  'grandparents': None,\n",
       "  'case': 'predict_hypernym'},\n",
       " {'children': 'epoll.n.1',\n",
       "  'parents': 'gnu linux.n.1',\n",
       "  'grandparents': None,\n",
       "  'case': 'predict_hypernym'},\n",
       " {'children': 'maximal information coefficient.n.1',\n",
       "  'parents': 'mutual information.n.1',\n",
       "  'grandparents': None,\n",
       "  'case': 'predict_hypernym'},\n",
       " {'children': 'smicrideinae.n.1',\n",
       "  'parents': 'hydropsychidae.n.1',\n",
       "  'grandparents': None,\n",
       "  'case': 'predict_hypernym'},\n",
       " {'children': 'point to point protocol over ethernet.n.1',\n",
       "  'parents': 'ethernet.n.1',\n",
       "  'grandparents': None,\n",
       "  'case': 'predict_hypernym'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/MAG_CS/test_hypernyms.pickle', 'wb') as f:\n",
    "    pickle.dump(test, f)\n",
    "\n",
    "with open('../data/MAG_CS/train_hypernyms.pickle', 'wb') as f:\n",
    "    pickle.dump(train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
