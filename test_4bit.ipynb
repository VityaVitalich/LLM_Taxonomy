{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/rabikov/anaconda3/envs/conda_taxonomy/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,3,6\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/raid/rabikov/hf_cache/'\n",
    "os.environ['HF_HOME'] = '/raid/rabikov/hf_cache/'\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_id = \"decapoda-research/llama-7b-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /raid/rabikov/anaconda3/envs/conda_taxonomy/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so\n",
      "CUDA SETUP: CUDA runtime path found: /raid/rabikov/anaconda3/envs/conda_taxonomy/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /raid/rabikov/anaconda3/envs/conda_taxonomy/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/rabikov/anaconda3/envs/conda_taxonomy/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/raid/rabikov/anaconda3/envs/conda_taxonomy/lib/libcudart.so'), PosixPath('/raid/rabikov/anaconda3/envs/conda_taxonomy/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "/raid/rabikov/anaconda3/envs/conda_taxonomy/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:20<00:00,  1.64it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "model = LlamaForCausalLM.from_pretrained(model_id,\n",
    "                                            quantization_config=bnb_config,\n",
    "                                            device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=16, \n",
    "   # target_modules=[\"query_key_value\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 3,504,607,232 || trainable%: 0.11967971650867153\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../NLP-DL-Project-hypo-to-hyper/pipeline_src/\")\n",
    "\n",
    "\n",
    "from config.config import TaskConfig\n",
    "from train import CustomScheduler, train\n",
    "from logger.logger import WanDBWriter\n",
    "from trainer.train_epoch import train_epoch, predict\n",
    "from dataset.dataset import init_data\n",
    "from logger.logger import WanDBWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import gc\n",
    "import wandb\n",
    "import json\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from accelerate import Accelerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator = Accelerator()\n",
    "\n",
    "accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TaskConfig()\n",
    "\n",
    "\n",
    "config.n_epochs = 1\n",
    "config.batch_size = 2\n",
    "config.lr = 3e-4\n",
    "config.min_lr = 3e-6\n",
    "\n",
    "config.validation = 1000\n",
    "config.save_every = 1000\n",
    "config.compute_metrics_every = 1000\n",
    "\n",
    "config.data_path = 'babel_datasets/wnet_only/reweighted_wnet_train_en_babel.pickle'\n",
    "config.gold_path = (\n",
    "    None  # \"SemEval2018-Task9/training/gold/1A.english.training.gold.txt\"\n",
    ")\n",
    "config.test_data_path = 'babel_datasets/wnet_only/reweighted_wnet_test_en_babel.pickle'\n",
    "config.test_gold_path = (\n",
    "    None  # \"SemEval2018-Task9/test/gold/1A.english.test.gold.txt\"\n",
    ")\n",
    "\n",
    "config.device = accelerator.device\n",
    "#config.device = 'cuda'\n",
    "config.using_peft = True\n",
    "config.model_type = \"Auto\"  # Auto or Llama\n",
    "config.wandb_log_dir = \"/raid/rabikov/wandb/\"\n",
    "config.model_checkpoint = \"EleutherAI/gpt-neo-125m\"\n",
    "config.exp_name = config.model_checkpoint.replace(\"/\", \"-\") + '_test_accelerate'\n",
    "config.saving_path = \"/raid/rabikov/model_checkpoints/\" + config.exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, train_loader, val_loader = init_data(tokenizer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=config.lr, betas=(0.9, 0.98), eps=1e-9\n",
    ")\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=config.lr,\n",
    "#                                                  steps_per_epoch=len(train_loader), epochs=config.n_epochs)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=len(train_loader) * config.n_epochs, eta_min=config.min_lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "for batch_idx, batch in pbar:\n",
    "\n",
    "\n",
    "    terms, att_mask_terms, targets, input_seqs, att_mask_input, labels = batch\n",
    "\n",
    "    output = model.forward(\n",
    "        input_seqs.to(config.device).long(),\n",
    "        attention_mask=att_mask_input.to(config.device).long(),\n",
    "        labels=labels.to(config.device).long(),\n",
    "    )\n",
    "    # output = model.forward(\n",
    "    #     input_seqs,\n",
    "    #     attention_mask=att_mask_input,\n",
    "    #     labels=labels,\n",
    "    # )\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = output[\"loss\"]\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model, optimizer, training_dataloader, scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_loader, scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7769 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 4818/7769 [1:47:44<1:05:59,  1.34s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 24\u001b[0m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mempty_cache()\n",
      "File \u001b[0;32m/raid/rabikov/anaconda3/envs/conda_taxonomy/lib/python3.10/site-packages/torch/cuda/memory.py:133\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Releases all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 133\u001b[0m     torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_emptyCache()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "for batch_idx, batch in pbar:\n",
    "\n",
    "\n",
    "    terms, att_mask_terms, targets, input_seqs, att_mask_input, labels = batch\n",
    "\n",
    "    # output = model.forward(\n",
    "    #     input_seqs.to(config.device).long(),\n",
    "    #     attention_mask=att_mask_input.to(config.device).long(),\n",
    "    #     labels=labels.to(config.device).long(),\n",
    "    # )\n",
    "    output = model.forward(\n",
    "        input_seqs,\n",
    "        attention_mask=att_mask_input,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = output[\"loss\"]\n",
    "    accelerator.backward(loss)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = tokenizer('hello how are you doing?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = tokenizer.encode('i am doing well', return_tensors='pt', add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 474,  626, 2599, 1532])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_seq \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mconcat([txt[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m], label], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
     ]
    }
   ],
   "source": [
    "input_seq = torch.concat([txt['input_ids'], label], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_term  = 'how are you?'\n",
    "target = \"great!\"\n",
    "\n",
    "\n",
    "encoded_term = tokenizer.encode(\n",
    "    processed_term, return_tensors='pt'\n",
    ")\n",
    "encoded_target = tokenizer.encode(target, return_tensors='pt', add_special_tokens=False)\n",
    "\n",
    "input_seq = torch.concat([encoded_term, encoded_target], dim=1)\n",
    "labels = input_seq.clone()\n",
    "labels[0, : encoded_term.size()[1]] = -100\n",
    "\n",
    "att_mask_inputs = torch.zeros_like(input_seq)\n",
    "att_mask_inputs[input_seq != 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ -100,  -100,  -100,  -100,  -100,  2107, 29991]]),\n",
       " tensor([[    0,   920,   526,   366, 29973,  2107, 29991]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.forward(input_ids=input_seq, attention_mask=att_mask_inputs, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.7479, grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.forward(input_ids=input_seq, attention_mask=att_mask_inputs, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.forward(input_ids=input_seq, attention_mask=att_mask_inputs, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.7083, grad_fn=<ToCopyBackward0>)\n",
      "tensor(5.6528, grad_fn=<ToCopyBackward0>)\n",
      "tensor(5.5960, grad_fn=<ToCopyBackward0>)\n",
      "tensor(5.5263, grad_fn=<ToCopyBackward0>)\n",
      "tensor(5.4481, grad_fn=<ToCopyBackward0>)\n",
      "tensor(5.3696, grad_fn=<ToCopyBackward0>)\n",
      "tensor(5.2882, grad_fn=<ToCopyBackward0>)\n",
      "tensor(5.1871, grad_fn=<ToCopyBackward0>)\n",
      "tensor(5.0791, grad_fn=<ToCopyBackward0>)\n",
      "tensor(4.9481, grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model.forward(input_ids=input_seq, attention_mask=att_mask_inputs, labels=labels)\n",
    "    loss = out.loss\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxonomy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
