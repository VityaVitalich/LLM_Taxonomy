{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"pipeline_src/\")\n",
    "\n",
    "from metrics.metrics import Metric\n",
    "from dataset.dataset import HypernymDataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from dataset.prompt_schemas import (\n",
    "    hypo_term_hyper,\n",
    "    predict_child_from_2_parents,\n",
    "    predict_child_from_parent,\n",
    "    predict_child_with_parent_and_grandparent,\n",
    "    predict_children_with_parent_and_brothers,\n",
    "    predict_parent_from_child_granparent,\n",
    "    predict_parent_from_child,\n",
    ")\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "pd.rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = '/home/home/v.moskvoretskii/NLP-DL-Project-hypo-to-hyper/babel_datasets/v2_wnet_test.pickle'\n",
    "path_pred = '/home/home/v.moskvoretskii/NLP-DL-Project-hypo-to-hyper/babel_datasets/_meta-llama-Llama-2-7b-hfwnet2_filtered_instruct_coma_0'\n",
    "\n",
    "with open(path_pred, 'rb') as f:\n",
    "    preds = pickle.load(f)\n",
    "\n",
    "with open(path_test, 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "len(preds), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1274/1274 [12:41<00:00,  1.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from translate import Translator\n",
    "translator= Translator(from_lang=\"en\",to_lang=\"ru\")\n",
    "\n",
    "def delete_techniqal(elem, remove=True):\n",
    "    if isinstance(elem, str):\n",
    "        if \".n.\" in elem and remove:\n",
    "            return elem.split(\".\")[0].replace(\"_\", \" \")\n",
    "        else:\n",
    "            return elem.replace(\"_\", \" \")\n",
    "\n",
    "    elif isinstance(elem, list):\n",
    "        new_words = []\n",
    "        for word in elem:\n",
    "            new_words.append(delete_techniqal(word, remove))\n",
    "        return new_words\n",
    "\n",
    "if not True:\n",
    "    res = {'parent': [], 'definition': [], 'children': [], 'translation': []}\n",
    "    for elem in tqdm(test):\n",
    "        case = elem[\"case\"]\n",
    "    # processed_term, target = transforms[case](elem)\n",
    "        \n",
    "        if isinstance(elem['parents'], list):\n",
    "            try:\n",
    "                definition = wn.synset(elem['parents'][0]).definition()\n",
    "            except:\n",
    "                definition = '-'\n",
    "            parents = ','.join(delete_techniqal(elem['parents']))\n",
    "        else:\n",
    "            try:        \n",
    "                definition = wn.synset(elem['parents']).definition()\n",
    "            except:\n",
    "                definition = '-'\n",
    "            parents = delete_techniqal(elem['parents'])\n",
    "        \n",
    "\n",
    "        if isinstance(elem['children'], list):\n",
    "            children = ','.join(delete_techniqal(elem['children']))\n",
    "        else:\n",
    "            children = delete_techniqal(elem['children'])\n",
    "\n",
    "        translation = translator.translate(children)\n",
    "\n",
    "        \n",
    "        \n",
    "        res['parent'].append(parents)\n",
    "        res['definition'].append(definition)\n",
    "        res['children'].append(children)\n",
    "        res['translation'].append(translation)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame.from_dict(res)\n",
    "    df['translation'][df['translation'].str.contains('MYMEMORY')] = ''\n",
    "    df['label'] = -1\n",
    "    df.to_csv('all_test_data.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('all_test_data.csv')\n",
    "# intersection_df = df1.sample(200, random_state=seed)\n",
    "# intersection_df.to_csv('intersection.csv', index=0)\n",
    "\n",
    "non_intersect = df1.drop(intersection_df.index)\n",
    "f,s,t = np.array_split(non_intersect, 3)\n",
    "\n",
    "fi = pd.concat([f, intersection_df])\n",
    "si = pd.concat([s, intersection_df])\n",
    "ti = pd.concat([t, intersection_df])\n",
    "\n",
    "# fi.to_excel('first_part2.xlsx')\n",
    "# si.to_excel('second_part.xlsx')\n",
    "# ti.to_excel('third_part.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxonomy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
