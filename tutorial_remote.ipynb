{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9ba5ee78-7790-4132-b6d5-da8cf9a93f36",
      "metadata": {
        "id": "9ba5ee78-7790-4132-b6d5-da8cf9a93f36"
      },
      "source": [
        "Requirements.txt\n",
        "\n",
        "torch 2.0.0+cu117\n",
        "transformers 4.28.1\n",
        "numpy\n",
        "typing\n",
        "tqdm\n",
        "gc\n",
        "pandas\n",
        "collections\n",
        "sklearn\n",
        "contextlib\n",
        "io\n",
        "IPython\n",
        "os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c49ca14d-cbec-4131-a1c7-b22e97aac112",
      "metadata": {
        "id": "c49ca14d-cbec-4131-a1c7-b22e97aac112"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [Errno 28] No space left on device. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3,4,6\"\n",
        "sys.path.append(\"../NLP-DL-Project-hypo-to-hyper/pipeline_src/\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device='cuda'\n",
        "    print('GPU')\n",
        "else:\n",
        "    device='cpu'\n",
        "    print('CPU')\n",
        "    \n",
        "    \n",
        "SEED = 0\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "print(torch.cuda.device_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15c037be-49a4-43b2-8d53-467bae409e9b",
      "metadata": {
        "id": "15c037be-49a4-43b2-8d53-467bae409e9b"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [Errno 28] No space left on device. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "path_data_en = \"SemEval2018-Task9/training/data/1A.english.training.data.txt\"\n",
        "path_gold_en = \"SemEval2018-Task9/training/gold/1A.english.training.gold.txt\"\n",
        "\n",
        "path_test_data_en = \"SemEval2018-Task9/test/data/1A.english.test.data.txt\"\n",
        "path_test_gold_en = \"SemEval2018-Task9/test/gold/1A.english.test.gold.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w9HiKC8qvUtD",
      "metadata": {
        "id": "w9HiKC8qvUtD"
      },
      "outputs": [],
      "source": [
        "# names = [path_data_en, path_gold_en, path_test_data_en, path_test_gold_en]\n",
        "# new_names = ['data_subset', 'gold_subset', 'test_subset', 'test_gold_subset']\n",
        "\n",
        "# for i, name in enumerate(names):\n",
        "#     try: \n",
        "#         df = pd.read_csv(name, sep='\\t', header=None)\n",
        "#     except pd.errors.ParserError:\n",
        "#         df = pd.read_csv(name, header=None, names=[\"hypernym\"])\n",
        "#     df1 = df.loc[:16]\n",
        "\n",
        "#     new_name = new_names[i] + '.txt'\n",
        "\n",
        "#     df1.to_csv(new_name, header=None, sep='\\t', index=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "71a19be5-3df1-418f-a0c3-665bd22637eb",
      "metadata": {
        "id": "71a19be5-3df1-418f-a0c3-665bd22637eb"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oJpo5S6P-_0g",
      "metadata": {
        "id": "oJpo5S6P-_0g"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [Errno 28] No space left on device. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from config.config import TaskConfig\n",
        "from train import CustomScheduler, train\n",
        "from logger.logger import WanDBWriter\n",
        "from trainer.train_epoch import train_epoch, predict\n",
        "from metrics.metrics import get_all_metrics\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        ")\n",
        "from config.config import TaskConfig\n",
        "import numpy as np\n",
        "from trainer.train_epoch import train_epoch, predict\n",
        "from metrics.metrics import get_all_metrics\n",
        "from torch.utils.data import DataLoader\n",
        "from dataset.dataset import HypernymDataset, Collator\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "import wandb\n",
        "from logger.logger import WanDBWriter\n",
        "\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa21d6ff",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
            "WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
            "CUDA_SETUP: Loading binary /srv/home/rabikov/taxonomy_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('FILE')}\n",
            "WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
            "WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
            "/srv/home/rabikov/taxonomy_env/lib/python3.8/site-packages/bitsandbytes/cextension.py:43: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        "    prepare_model_for_int8_training,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54XsYWrN_MAL",
      "metadata": {
        "id": "54XsYWrN_MAL"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [Errno 28] No space left on device. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "config = TaskConfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_MLP1Ip0_Wmy",
      "metadata": {
        "id": "_MLP1Ip0_Wmy"
      },
      "outputs": [],
      "source": [
        "config.batch_size = 4\n",
        "config.embedding_dim = 2048\n",
        "config.validation = 240\n",
        "config.data_path = path_data_en\n",
        "config.gold_path = path_gold_en\n",
        "config.test_data_path = path_test_data_en\n",
        "config.test_gold_path = path_test_gold_en\n",
        "config.device = device\n",
        "config.using_peft = True\n",
        "config.n_epochs = 10\n",
        "config.lr = 3e-4\n",
        "config.min_lr = 3e-6\n",
        "\n",
        "\n",
        "config.wandb_log_dir = '/raid/rabikov/wandb/'\n",
        "config.exp_name = 'eachadea-vicuna-7b-1.1'\n",
        "config.model_checkpoint = 'databricks/dolly-v2-7b'\n",
        "config.saving_path = '/raid/rabikov/model_checkpoints/eachadea-vicuna-7b-1.1'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vZW9jlOV_WvV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264,
          "referenced_widgets": [
            "32a03fd30cee4b67af5bd7a1729ae96f",
            "cf8936b2f9b94452bfcd2b66ad6b405b",
            "8cf6f3dc53d74da7813fb8cd8924438e",
            "1512545756a84b759c02c85eff980fd1",
            "9a2bc584900942b8aa8a35b70ab463e5",
            "5d1f3cb411f84ea6a87ecec032e4c4e0",
            "0e4c95c75e1e4756826b09c5afad0fb5",
            "229a208f05fa4168b07c3bb3ce522dcb",
            "c7b0f4da4e624f37998c6c55db761ad4",
            "3426456a50134da589ba163fdd079ce0",
            "3fc22325c69e4292a3d2fd002ae4c471",
            "0eaa66a2acc34675863228355a166f7f",
            "cfa5bea3cae74696a01c37a0b5726509",
            "90e2a4749d2a438ea396ac2a60fb40f7",
            "487c84226af5476d8a2a32b0f79acacd",
            "5df817497b5547cf951e1c318eeb9535",
            "e50c7aefdccc4c4e9998e24ae56cf7c2",
            "2eca7409d1b84c178fe6d3aada1dc9f5",
            "5726d739909d433fae883a3eadcb8337",
            "b7c1e4db4a0f4e9db46e2882df1585d2",
            "5072d9eda7e24331874f02f5951141e4",
            "a49a8799b49344bdabf11b20657cfa05",
            "686e0a152c4646c3b68fb699935fcaa1",
            "516ffbad03ae4cafac46641cb399aa78",
            "aa360f9723ca46f3a2593ee82ee504d2",
            "b842655cd19347218921880fbbc38a10",
            "aad5957ff560488ab2f292e2ddcb70f8",
            "e2d361d0ba3649d59f6d1c5a3f2af985",
            "a1949c2fcd774558b90ec10efbd6d203",
            "df0c385b8ced4db288b0d6bad83e4a43",
            "4de9408cd05949a989fd9f24dc2714c0",
            "6c0e6e65751f466cb63caa6f8774b96a",
            "7d2d5f9eb7a744538cc42ef7ff5fe130",
            "83e9f453ceec4a8b90a4b730648b7941",
            "35e2bddbdffe41939a9dcd88dc965e92",
            "a4b9be85fd1140b5985803b5dd228477",
            "17278cd2d15f49c1b01d9b5863158778",
            "64e33ff7558c49baa8244928802ef3f5",
            "19227bef944244a2999a2930a9238146",
            "3a9f1743e44e4697946ea275a8e25c56",
            "9ea2e8f84ab24e4d8e9bb3f93b170c0d",
            "34802e62dc4c4887af3b9396b7ebb737",
            "96062273d13f423f930bdc02eb45a6c8",
            "282e1bb49fd849ae91d40fa79bffdb6f",
            "d369e06656544bef8002ef8c6e2e18d8",
            "62ec49a15c394574bc68482c3c90eec3",
            "437bff5f3034482a91c7dcf3f3bc9d92",
            "5dba750a5b5c42b4b9f1d34683347660",
            "c0c6995b4f3748bd814e93ea9524acef",
            "68efda37e1f4480f8d4a2a9a8c1d1d51",
            "4acb74c5858f44a3ad5126b158cc9e47",
            "f7dfeda09dd9420db74e6b5ab63f02fc",
            "635ec7b3dc574637aaca674f976b11ad",
            "2188d5d7d9b540aba4609e8c4b756fdb",
            "3662ab3f3c694063b4543a59dfac236e",
            "e0ce0b0ae3ba4f1bacad72af72327864",
            "0369c69255434eb5920af69c6b0eab30",
            "d82f1e939d3c422388a5cd1cd52dd581",
            "ddf34cc7cde642d3a93dd564ed11a944",
            "2b5300afd4544140aa6119c705322c69",
            "9a66592bb7f84d43ac0261d1306b908b",
            "5942d744160e4705b50f48617fb19c80",
            "d7284aac32914833a0390326219265a8",
            "af0c50c6c7444efab168ef8bd0d45e20",
            "e850b4f10701493cb8a9b8173bf02b1d",
            "7bd952607b0b4a44a4e7da152c2bbb36"
          ]
        },
        "id": "vZW9jlOV_WvV",
        "outputId": "03a7dcd3-b9f3-4e63-a4f4-0907b3277310"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [Errno 28] No space left on device. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(config.model_checkpoint,\n",
        "    #load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df0f3596",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [Errno 28] No space left on device. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    config.model_checkpoint,\n",
        "    padding_side=\"left\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ea17e27",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [Errno 28] No space left on device. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "LORA_R = 8\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT= 0.05\n",
        "LORA_TARGET_MODULES = [\n",
        "    \"q\",\n",
        "    \"v\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8462142f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 4194304 || all params: 6860251136 || trainable%: 0.061139219495768615\n"
          ]
        }
      ],
      "source": [
        "#model = prepare_model_for_int8_training(model)\n",
        "config_lora = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "   # target_modules=LORA_TARGET_MODULES,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, config_lora)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8205fbbb",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [Errno 28] No space left on device. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "\n",
        "# data\n",
        "train_dataset = HypernymDataset(\n",
        "    data_path=config.data_path,\n",
        "    tokenizer=tokenizer,\n",
        "    gold_path=config.gold_path,\n",
        "    semeval_format=True,\n",
        ")\n",
        "test_dataset = HypernymDataset(\n",
        "    data_path=config.test_data_path,\n",
        "    tokenizer=tokenizer,\n",
        "    gold_path=config.test_gold_path,\n",
        "    semeval_format=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39773d3e",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [Errno 28] No space left on device. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "\n",
        "collator = Collator(tokenizer.eos_token_id, tokenizer.eos_token_id, -100)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    collate_fn=collator,\n",
        "    shuffle=True,\n",
        "    num_workers=8,\n",
        "    drop_last=True,\n",
        "    pin_memory=True,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    collate_fn=collator,\n",
        "    shuffle=False,\n",
        "    num_workers=8,\n",
        "    drop_last=True,\n",
        "    pin_memory=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "EoNLV-KsB7XT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "EoNLV-KsB7XT",
        "outputId": "65e55f9d-76f2-4ced-ee3e-0c24fed3666b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvityavitalich\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.2 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/raid/rabikov/wandb/wandb/run-20230507_172918-yz8bnjyw</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/vityavitalich/taxonomy/runs/yz8bnjyw' target=\"_blank\">eachadea-vicuna-7b-1.1</a></strong> to <a href='https://wandb.ai/vityavitalich/taxonomy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/vityavitalich/taxonomy' target=\"_blank\">https://wandb.ai/vityavitalich/taxonomy</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/vityavitalich/taxonomy/runs/yz8bnjyw' target=\"_blank\">https://wandb.ai/vityavitalich/taxonomy/runs/yz8bnjyw</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(), lr=config.lr, betas=(0.9, 0.98), eps=1e-9\n",
        ")\n",
        "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=config.lr,\n",
        "#                                                  steps_per_epoch=len(train_loader), epochs=config.n_epochs)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader) * config.n_epochs,\n",
        "                                                        eta_min=config.min_lr)\n",
        "\n",
        "# wandb\n",
        "logger = WanDBWriter(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "M3ix-WuyB_Ds",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3ix-WuyB_Ds",
        "outputId": "43cd58cd-7470-4e55-ff3b-610b8700d169"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start of the epoch 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 375/375 [06:04<00:00,  1.03it/s, Loss=2.03]  \n",
            "eval going: 100%|██████████| 375/375 [06:02<00:00,  1.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start of the epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 375/375 [06:05<00:00,  1.03it/s, Loss=1.9]   \n",
            "eval going: 100%|██████████| 375/375 [06:07<00:00,  1.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start of the epoch 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 375/375 [06:06<00:00,  1.02it/s, Loss=1.32]  \n",
            "eval going: 100%|██████████| 375/375 [06:12<00:00,  1.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start of the epoch 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 375/375 [06:04<00:00,  1.03it/s, Loss=0.678]  \n",
            "eval going: 100%|██████████| 375/375 [06:12<00:00,  1.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start of the epoch 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 375/375 [06:06<00:00,  1.02it/s, Loss=0.698]  \n",
            "eval going: 100%|██████████| 375/375 [06:14<00:00,  1.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start of the epoch 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 375/375 [06:01<00:00,  1.04it/s, Loss=1.36]   \n",
            "eval going: 100%|██████████| 375/375 [06:01<00:00,  1.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start of the epoch 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 375/375 [06:06<00:00,  1.02it/s, Loss=0.496]  \n",
            "eval going: 100%|██████████| 375/375 [06:01<00:00,  1.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start of the epoch 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 375/375 [06:05<00:00,  1.03it/s, Loss=0.122]  \n",
            "eval going: 100%|██████████| 375/375 [06:03<00:00,  1.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start of the epoch 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 375/375 [06:05<00:00,  1.03it/s, Loss=0.117]  \n",
            "eval going: 100%|██████████| 375/375 [06:05<00:00,  1.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start of the epoch 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 375/375 [06:07<00:00,  1.02it/s, Loss=0.177]   \n",
            "eval going: 100%|██████████| 375/375 [06:16<00:00,  1.00s/it]\n"
          ]
        }
      ],
      "source": [
        "train(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        criterion,\n",
        "        logger,\n",
        "        config,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8d9213b",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [Errno 28] No space left on device. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "load_path = '/raid/rabikov/model_checkpoints/databricks-dolly-v2-7b_epoch=0_MAP=0.10859000826334173.pth'\n",
        "checkpoint = torch.load(load_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b60e140",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mOSError: [Errno 28] No space left on device. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "\n",
        "model.load_state_dict(checkpoint['model'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "78bffa40",
      "metadata": {},
      "outputs": [],
      "source": [
        "del checkpoint\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3acbb976",
      "metadata": {},
      "outputs": [],
      "source": [
        "for batch in val_loader:\n",
        "    terms, att_mask_terms, targets, input_seqs, att_mask_input, labels = batch\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "cdde46b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "config.gen_args = {\"no_repeat_ngram_size\":2,\n",
        "                    \"max_new_tokens\":32, \n",
        "                    \"num_return_sequences\": 2,\n",
        "                    \"num_beams\": 15,\n",
        "                    \"early_stopping\":True, \n",
        "                    \"num_beam_groups\":5, \n",
        "                    \"diversity_penalty\":1.0,\n",
        "                    \"temperature\": 0.9}\n",
        "\n",
        "\n",
        "config.gen_args = {\n",
        "            \"num_beams\": 15,\n",
        "            \"early_stopping\": True,\n",
        "            \"max_new_tokens\": 16,\n",
        "            \"temperature\": 0.9\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "0ad68761",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "eval going:   0%|          | 0/375 [00:04<?, ?it/s]\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 8.86 GiB already allocated; 12.56 MiB free; 10.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_labels, all_preds \u001b[39m=\u001b[39m predict(model\u001b[39m.\u001b[39;49mmodel, tokenizer, val_loader, config)\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/NLP-DL-Project-hypo-to-hyper/../NLP-DL-Project-hypo-to-hyper/pipeline_src/trainer/train_epoch.py:124\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, tokenizer, val_loader, config)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, batch \u001b[39min\u001b[39;00m evalbar:\n\u001b[1;32m    122\u001b[0m     terms, att_mask_terms, targets, input_seqs, att_mask_input, labels \u001b[39m=\u001b[39m batch\n\u001b[0;32m--> 124\u001b[0m     output_tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    125\u001b[0m         terms\u001b[39m.\u001b[39;49mto(config\u001b[39m.\u001b[39;49mdevice),\n\u001b[1;32m    126\u001b[0m         attention_mask\u001b[39m=\u001b[39;49matt_mask_terms\u001b[39m.\u001b[39;49mto(config\u001b[39m.\u001b[39;49mdevice),\n\u001b[1;32m    127\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m    128\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig\u001b[39m.\u001b[39;49mgen_args,\n\u001b[1;32m    129\u001b[0m     )\n\u001b[1;32m    130\u001b[0m     pred_tokens \u001b[39m=\u001b[39m output_tokens[:, terms\u001b[39m.\u001b[39msize()[\u001b[39m1\u001b[39m] :]\n\u001b[1;32m    131\u001b[0m     pred_str \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(pred_tokens\u001b[39m.\u001b[39mcpu(), skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/transformers/generation/utils.py:1524\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1518\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1519\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1520\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1521\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1522\u001b[0m     )\n\u001b[1;32m   1523\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1524\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1525\u001b[0m         input_ids,\n\u001b[1;32m   1526\u001b[0m         beam_scorer,\n\u001b[1;32m   1527\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1528\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1529\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1530\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1531\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1532\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1533\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1534\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1535\u001b[0m     )\n\u001b[1;32m   1537\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1538\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1539\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/transformers/generation/utils.py:2883\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2879\u001b[0m model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   2880\u001b[0m     outputs, model_kwargs, is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder\n\u001b[1;32m   2881\u001b[0m )\n\u001b[1;32m   2882\u001b[0m \u001b[39mif\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mpast_key_values\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2883\u001b[0m     model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mpast_key_values\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reorder_cache(model_kwargs[\u001b[39m\"\u001b[39;49m\u001b[39mpast_key_values\u001b[39;49m\u001b[39m\"\u001b[39;49m], beam_idx)\n\u001b[1;32m   2885\u001b[0m \u001b[39mif\u001b[39;00m return_dict_in_generate \u001b[39mand\u001b[39;00m output_scores:\n\u001b[1;32m   2886\u001b[0m     beam_indices \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m((beam_indices[beam_idx[i]] \u001b[39m+\u001b[39m (beam_idx[i],) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(beam_indices))))\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:730\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM._reorder_cache\u001b[0;34m(self, past_key_values, beam_idx)\u001b[0m\n\u001b[1;32m    727\u001b[0m reordered_past \u001b[39m=\u001b[39m ()\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m layer_past \u001b[39min\u001b[39;00m past_key_values:\n\u001b[1;32m    729\u001b[0m     reordered_past \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m--> 730\u001b[0m         \u001b[39mtuple\u001b[39;49m(past_state\u001b[39m.\u001b[39;49mindex_select(\u001b[39m0\u001b[39;49m, beam_idx) \u001b[39mfor\u001b[39;49;00m past_state \u001b[39min\u001b[39;49;00m layer_past[:\u001b[39m2\u001b[39;49m]) \u001b[39m+\u001b[39m layer_past[\u001b[39m2\u001b[39m:],\n\u001b[1;32m    731\u001b[0m     )\n\u001b[1;32m    732\u001b[0m \u001b[39mreturn\u001b[39;00m reordered_past\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:730\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    727\u001b[0m reordered_past \u001b[39m=\u001b[39m ()\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m layer_past \u001b[39min\u001b[39;00m past_key_values:\n\u001b[1;32m    729\u001b[0m     reordered_past \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m--> 730\u001b[0m         \u001b[39mtuple\u001b[39m(past_state\u001b[39m.\u001b[39;49mindex_select(\u001b[39m0\u001b[39;49m, beam_idx) \u001b[39mfor\u001b[39;00m past_state \u001b[39min\u001b[39;00m layer_past[:\u001b[39m2\u001b[39m]) \u001b[39m+\u001b[39m layer_past[\u001b[39m2\u001b[39m:],\n\u001b[1;32m    731\u001b[0m     )\n\u001b[1;32m    732\u001b[0m \u001b[39mreturn\u001b[39;00m reordered_past\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 8.86 GiB already allocated; 12.56 MiB free; 10.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "all_labels, all_preds = predict(model.model, tokenizer, val_loader, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "c8413f64",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm \n",
        "import itertools\n",
        "from collections import Counter\n",
        "import pickle\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_multiple(model, tokenizer, val_loader, config):\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    evalbar = tqdm(enumerate(val_loader), total=len(val_loader), desc=\"eval going\")\n",
        "    for batch_idx, batch in evalbar:\n",
        "        terms, att_mask_terms, targets, input_seqs, att_mask_input, labels = batch\n",
        "\n",
        "        output_tokens = model.generate(\n",
        "            terms.to(config.device),\n",
        "            attention_mask=att_mask_terms.to(config.device),\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            **config.gen_args,\n",
        "        )\n",
        "        pred_tokens = output_tokens[:, terms.size()[1] :]\n",
        "        pred_str = tokenizer.batch_decode(pred_tokens.cpu(), skip_special_tokens=True)\n",
        "        gold_str = tokenizer.batch_decode(targets, skip_special_tokens=True)\n",
        "\n",
        "        del output_tokens\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        merged_iter = itertools.chain.from_iterable(list(map(lambda x: x.split(','), pred_str)))\n",
        "        sorted_predicted_answer = [i[0].strip().replace('\\n', '') for i in Counter(merged_iter).most_common()]\n",
        "\n",
        "        all_preds.append(sorted_predicted_answer)\n",
        "        all_labels.extend(gold_str)\n",
        "\n",
        "        if batch_idx % 5 == 0:\n",
        "            with open('/raid/rabikov/model_outputs/predictions_dolly_2', 'wb') as fp:\n",
        "                pickle.dump(all_preds, fp)\n",
        "        \n",
        "\n",
        "    return all_preds, all_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c58c419",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "c9f5ee2b",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "eval going:   1%|          | 12/1500 [01:58<4:05:49,  9.91s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[89], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_preds, all_labels \u001b[39m=\u001b[39m predict_multiple(model\u001b[39m.\u001b[39;49mmodel, tokenizer, val_loader, config)\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "Cell \u001b[0;32mIn[88], line 18\u001b[0m, in \u001b[0;36mpredict_multiple\u001b[0;34m(model, tokenizer, val_loader, config)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, batch \u001b[39min\u001b[39;00m evalbar:\n\u001b[1;32m     16\u001b[0m     terms, att_mask_terms, targets, input_seqs, att_mask_input, labels \u001b[39m=\u001b[39m batch\n\u001b[0;32m---> 18\u001b[0m     output_tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     19\u001b[0m         terms\u001b[39m.\u001b[39;49mto(config\u001b[39m.\u001b[39;49mdevice),\n\u001b[1;32m     20\u001b[0m         attention_mask\u001b[39m=\u001b[39;49matt_mask_terms\u001b[39m.\u001b[39;49mto(config\u001b[39m.\u001b[39;49mdevice),\n\u001b[1;32m     21\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     22\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig\u001b[39m.\u001b[39;49mgen_args,\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m     pred_tokens \u001b[39m=\u001b[39m output_tokens[:, terms\u001b[39m.\u001b[39msize()[\u001b[39m1\u001b[39m] :]\n\u001b[1;32m     25\u001b[0m     pred_str \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(pred_tokens\u001b[39m.\u001b[39mcpu(), skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/transformers/generation/utils.py:1609\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1602\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1603\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1604\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1605\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1606\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1607\u001b[0m     )\n\u001b[1;32m   1608\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1609\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroup_beam_search(\n\u001b[1;32m   1610\u001b[0m         input_ids,\n\u001b[1;32m   1611\u001b[0m         beam_scorer,\n\u001b[1;32m   1612\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1613\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1614\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1615\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1616\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1617\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1618\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1619\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1620\u001b[0m     )\n\u001b[1;32m   1622\u001b[0m \u001b[39melif\u001b[39;00m is_constraint_gen_mode:\n\u001b[1;32m   1623\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m generation_config\u001b[39m.\u001b[39mnum_beams:\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/transformers/generation/utils.py:3481\u001b[0m, in \u001b[0;36mGenerationMixin.group_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3479\u001b[0m \u001b[39m# do one decoder step on all beams of all sentences in batch\u001b[39;00m\n\u001b[1;32m   3480\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 3481\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   3482\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   3483\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   3484\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   3485\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   3486\u001b[0m )\n\u001b[1;32m   3488\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3489\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39;49m_hf_hook\u001b[39m.\u001b[39;49mpost_forward(module, output)\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/accelerate/hooks.py:292\u001b[0m, in \u001b[0;36mAlignDevicesHook.post_forward\u001b[0;34m(self, module, output)\u001b[0m\n\u001b[1;32m    289\u001b[0m         set_module_tensor_to_device(module, name, \u001b[39m\"\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    291\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mio_same_device \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 292\u001b[0m     output \u001b[39m=\u001b[39m send_to_device(output, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_device)\n\u001b[1;32m    294\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/accelerate/utils/operations.py:133\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_has_to_method\u001b[39m(t):\n\u001b[1;32m    131\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mhasattr\u001b[39m(t, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 133\u001b[0m \u001b[39mreturn\u001b[39;00m recursively_apply(_send_to_device, tensor, device, non_blocking, test_type\u001b[39m=\u001b[39;49m_has_to_method)\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/accelerate/utils/operations.py:93\u001b[0m, in \u001b[0;36mrecursively_apply\u001b[0;34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[39mreturn\u001b[39;00m honor_type(\n\u001b[1;32m     83\u001b[0m         data,\n\u001b[1;32m     84\u001b[0m         (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m         ),\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     91\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Mapping):\n\u001b[1;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)(\n\u001b[0;32m---> 93\u001b[0m         {\n\u001b[1;32m     94\u001b[0m             k: recursively_apply(\n\u001b[1;32m     95\u001b[0m                 func, v, \u001b[39m*\u001b[39margs, test_type\u001b[39m=\u001b[39mtest_type, error_on_other_type\u001b[39m=\u001b[39merror_on_other_type, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m     96\u001b[0m             )\n\u001b[1;32m     97\u001b[0m             \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mitems()\n\u001b[1;32m     98\u001b[0m         }\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    100\u001b[0m \u001b[39melif\u001b[39;00m test_type(data):\n\u001b[1;32m    101\u001b[0m     \u001b[39mreturn\u001b[39;00m func(data, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/accelerate/utils/operations.py:94\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[39mreturn\u001b[39;00m honor_type(\n\u001b[1;32m     83\u001b[0m         data,\n\u001b[1;32m     84\u001b[0m         (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m         ),\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     91\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Mapping):\n\u001b[1;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)(\n\u001b[1;32m     93\u001b[0m         {\n\u001b[0;32m---> 94\u001b[0m             k: recursively_apply(\n\u001b[1;32m     95\u001b[0m                 func, v, \u001b[39m*\u001b[39;49margs, test_type\u001b[39m=\u001b[39;49mtest_type, error_on_other_type\u001b[39m=\u001b[39;49merror_on_other_type, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m     96\u001b[0m             )\n\u001b[1;32m     97\u001b[0m             \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mitems()\n\u001b[1;32m     98\u001b[0m         }\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    100\u001b[0m \u001b[39melif\u001b[39;00m test_type(data):\n\u001b[1;32m    101\u001b[0m     \u001b[39mreturn\u001b[39;00m func(data, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/accelerate/utils/operations.py:82\u001b[0m, in \u001b[0;36mrecursively_apply\u001b[0;34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39mRecursively apply a function on a data structure that is a nested list/tuple/dictionary of a given base type.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39m    The same data structure as `data` with `func` applied to every object of type `main_type`.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)):\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mreturn\u001b[39;00m honor_type(\n\u001b[1;32m     83\u001b[0m         data,\n\u001b[1;32m     84\u001b[0m         (\n\u001b[1;32m     85\u001b[0m             recursively_apply(\n\u001b[1;32m     86\u001b[0m                 func, o, \u001b[39m*\u001b[39;49margs, test_type\u001b[39m=\u001b[39;49mtest_type, error_on_other_type\u001b[39m=\u001b[39;49merror_on_other_type, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m     87\u001b[0m             )\n\u001b[1;32m     88\u001b[0m             \u001b[39mfor\u001b[39;49;00m o \u001b[39min\u001b[39;49;00m data\n\u001b[1;32m     89\u001b[0m         ),\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     91\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Mapping):\n\u001b[1;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)(\n\u001b[1;32m     93\u001b[0m         {\n\u001b[1;32m     94\u001b[0m             k: recursively_apply(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m         }\n\u001b[1;32m     99\u001b[0m     )\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/accelerate/utils/operations.py:53\u001b[0m, in \u001b[0;36mhonor_type\u001b[0;34m(obj, generator)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mCast a generator to the same type as obj (list, tuple, or namedtuple)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39;49m(obj)(generator)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     \u001b[39m# Some objects may not be able to instantiate from a generator directly\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(obj)(\u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(generator))\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/accelerate/utils/operations.py:85\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39mRecursively apply a function on a data structure that is a nested list/tuple/dictionary of a given base type.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39m    The same data structure as `data` with `func` applied to every object of type `main_type`.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)):\n\u001b[1;32m     82\u001b[0m     \u001b[39mreturn\u001b[39;00m honor_type(\n\u001b[1;32m     83\u001b[0m         data,\n\u001b[1;32m     84\u001b[0m         (\n\u001b[0;32m---> 85\u001b[0m             recursively_apply(\n\u001b[1;32m     86\u001b[0m                 func, o, \u001b[39m*\u001b[39;49margs, test_type\u001b[39m=\u001b[39;49mtest_type, error_on_other_type\u001b[39m=\u001b[39;49merror_on_other_type, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m     87\u001b[0m             )\n\u001b[1;32m     88\u001b[0m             \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m data\n\u001b[1;32m     89\u001b[0m         ),\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     91\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Mapping):\n\u001b[1;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)(\n\u001b[1;32m     93\u001b[0m         {\n\u001b[1;32m     94\u001b[0m             k: recursively_apply(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m         }\n\u001b[1;32m     99\u001b[0m     )\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/accelerate/utils/operations.py:82\u001b[0m, in \u001b[0;36mrecursively_apply\u001b[0;34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39mRecursively apply a function on a data structure that is a nested list/tuple/dictionary of a given base type.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39m    The same data structure as `data` with `func` applied to every object of type `main_type`.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)):\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mreturn\u001b[39;00m honor_type(\n\u001b[1;32m     83\u001b[0m         data,\n\u001b[1;32m     84\u001b[0m         (\n\u001b[1;32m     85\u001b[0m             recursively_apply(\n\u001b[1;32m     86\u001b[0m                 func, o, \u001b[39m*\u001b[39;49margs, test_type\u001b[39m=\u001b[39;49mtest_type, error_on_other_type\u001b[39m=\u001b[39;49merror_on_other_type, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m     87\u001b[0m             )\n\u001b[1;32m     88\u001b[0m             \u001b[39mfor\u001b[39;49;00m o \u001b[39min\u001b[39;49;00m data\n\u001b[1;32m     89\u001b[0m         ),\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     91\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Mapping):\n\u001b[1;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)(\n\u001b[1;32m     93\u001b[0m         {\n\u001b[1;32m     94\u001b[0m             k: recursively_apply(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m         }\n\u001b[1;32m     99\u001b[0m     )\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/accelerate/utils/operations.py:53\u001b[0m, in \u001b[0;36mhonor_type\u001b[0;34m(obj, generator)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mCast a generator to the same type as obj (list, tuple, or namedtuple)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39;49m(obj)(generator)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     \u001b[39m# Some objects may not be able to instantiate from a generator directly\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(obj)(\u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(generator))\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/accelerate/utils/operations.py:85\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39mRecursively apply a function on a data structure that is a nested list/tuple/dictionary of a given base type.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39m    The same data structure as `data` with `func` applied to every object of type `main_type`.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)):\n\u001b[1;32m     82\u001b[0m     \u001b[39mreturn\u001b[39;00m honor_type(\n\u001b[1;32m     83\u001b[0m         data,\n\u001b[1;32m     84\u001b[0m         (\n\u001b[0;32m---> 85\u001b[0m             recursively_apply(\n\u001b[1;32m     86\u001b[0m                 func, o, \u001b[39m*\u001b[39;49margs, test_type\u001b[39m=\u001b[39;49mtest_type, error_on_other_type\u001b[39m=\u001b[39;49merror_on_other_type, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m     87\u001b[0m             )\n\u001b[1;32m     88\u001b[0m             \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m data\n\u001b[1;32m     89\u001b[0m         ),\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     91\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Mapping):\n\u001b[1;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)(\n\u001b[1;32m     93\u001b[0m         {\n\u001b[1;32m     94\u001b[0m             k: recursively_apply(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m         }\n\u001b[1;32m     99\u001b[0m     )\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/accelerate/utils/operations.py:101\u001b[0m, in \u001b[0;36mrecursively_apply\u001b[0;34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)(\n\u001b[1;32m     93\u001b[0m         {\n\u001b[1;32m     94\u001b[0m             k: recursively_apply(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m         }\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    100\u001b[0m \u001b[39melif\u001b[39;00m test_type(data):\n\u001b[0;32m--> 101\u001b[0m     \u001b[39mreturn\u001b[39;00m func(data, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    102\u001b[0m \u001b[39melif\u001b[39;00m error_on_other_type:\n\u001b[1;32m    103\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    104\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt apply \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m on object of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m, only of nested list/tuple/dicts of objects \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthat satisfy \u001b[39m\u001b[39m{\u001b[39;00mtest_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m     )\n",
            "File \u001b[0;32m~/taxonomy_env/lib/python3.8/site-packages/accelerate/utils/operations.py:126\u001b[0m, in \u001b[0;36msend_to_device.<locals>._send_to_device\u001b[0;34m(t, device, non_blocking)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_send_to_device\u001b[39m(t, device, non_blocking):\n\u001b[1;32m    125\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, non_blocking\u001b[39m=\u001b[39;49mnon_blocking)\n\u001b[1;32m    127\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:  \u001b[39m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[1;32m    128\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "all_preds, all_labels = predict_multiple(model.model, tokenizer, val_loader, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "fabcd537",
      "metadata": {},
      "outputs": [],
      "source": [
        "#','.join(all_preds[0])\n",
        "\n",
        "preds_str = list(map(lambda x: ','.join(x), all_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "712c23b1",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "b791b41e",
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = get_all_metrics(all_labels, preds_str, limit=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "704ba767",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'MRR': 0.29735392603932786,\n",
              " 'MAP': 0.20843206632142522,\n",
              " 'P@1': 0.22066666666666668,\n",
              " 'P@3': 0.17655555555555622,\n",
              " 'P@5': 0.17812222222222154,\n",
              " 'P@15': 0.21145639360639343}"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "444e84c5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'MRR': 0.2963895104895101,\n",
              " 'MAP': 0.19487119608786296,\n",
              " 'P@1': 0.22066666666666668,\n",
              " 'P@3': 0.17655555555555622,\n",
              " 'P@5': 0.17812222222222154,\n",
              " 'P@15': 0.2105809967809966}"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "59a19080",
      "metadata": {},
      "outputs": [],
      "source": [
        "from metrics.metrics import get_hypernyms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "c7e40d0f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_hypernyms(line, is_gold=True, limit=15):\n",
        "    if is_gold == True:\n",
        "        valid_hyps = line.strip().split(\",\")\n",
        "        return list(map(lambda x: x.strip(), valid_hyps))\n",
        "    else:\n",
        "        linesplit = line.strip().split(\",\")\n",
        "        cand_hyps = []\n",
        "        for hyp in linesplit[:limit]:\n",
        "            hyp_lower = hyp.lower().strip()\n",
        "            if hyp_lower not in cand_hyps:\n",
        "                cand_hyps.append(hyp_lower)\n",
        "        return cand_hyps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "cfffa69c",
      "metadata": {},
      "outputs": [],
      "source": [
        "golden = get_hypernyms(all_labels[3], is_gold=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "7f2a9085",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['proceedings',\n",
              " 'legal proceedings',\n",
              " 'proceeding',\n",
              " 'due process',\n",
              " 'legal proceeding']"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "golden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "4d69dd1d",
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = get_hypernyms(preds_str[3], is_gold=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "bc5b8ec5",
      "metadata": {},
      "outputs": [],
      "source": [
        "gold_hyps_n = len(golden)\n",
        "limit = 15\n",
        "r = [0 for i in range(limit)]\n",
        "\n",
        "for j in range(len(pred)):\n",
        "    pred_hyp = pred[j]\n",
        "    if pred_hyp in golden:\n",
        "        r[j] = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "d48b28d5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "0d585207",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['suit',\n",
              " 'lawsuit',\n",
              " 'legal action',\n",
              " 'court case',\n",
              " 'proceeding',\n",
              " 'process',\n",
              " 'litigation',\n",
              " 'trial',\n",
              " 'law suit.<!doctype html public \"-//w3c//dtd html 4.01 transitional//en\" \"http://www.w3.org/tr/html4/loose.dtd\">',\n",
              " 'law suit.<!doctype html public \"-//w3c//dtd html 4.01//en\" \"http://www.w3.org/tr/html4/strict.dtd\"><html',\n",
              " 'law suit.<!doctype html public \"-//w3c//dtd html 4.01//en\" \"http://www.w3.org/tr/html4/strict.dtd\"><']"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "8a136ada",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['proceedings',\n",
              " ' legal proceedings',\n",
              " ' proceeding',\n",
              " ' due process',\n",
              " ' legal proceeding']"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "golden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "2307fc41",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['one_ans\\n\\t', 'sec_ans']"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "itemlist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "59e7ae83",
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_iter = itertools.chain.from_iterable(list(map(lambda x: x.split(','), pred_str)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "5fbf1e3b",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "f6903d53",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "40b012f8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['malice',\n",
              " 'ill will',\n",
              " 'evil',\n",
              " 'wickedness',\n",
              " 'malevolence',\n",
              " 'mischief',\n",
              " 'hostility',\n",
              " 'spitefulness',\n",
              " 'bad feeling',\n",
              " 'hatred',\n",
              " 'enmity',\n",
              " 'dislike',\n",
              " 'animosity',\n",
              " 'grudge',\n",
              " 'rancor',\n",
              " 'resentment',\n",
              " 'anger',\n",
              " 'malice',\n",
              " 'antagonism',\n",
              " 'disliking',\n",
              " 'antipathy',\n",
              " 'aversion',\n",
              " '',\n",
              " 'opposition',\n",
              " 'prejudice']"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sorted_predicted_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "21643536",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['malevolence, distaste, hatred, hate, malignity']"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.batch_decode(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "baab9065",
      "metadata": {},
      "outputs": [],
      "source": [
        "del out \n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40ce45c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "tokenizer.encode(strings, return_tensors=\"pt\", add_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1cac99",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.encode(strings, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e41d74e",
      "metadata": {},
      "outputs": [],
      "source": [
        "batch[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dee3c86a-b668-436f-94b9-071d34acb826",
      "metadata": {
        "id": "dee3c86a-b668-436f-94b9-071d34acb826"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from dataset.prompt_schemas import hypo_term_hyper\n",
        "import pandas as pd\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "class HypernymDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_path,\n",
        "        tokenizer,\n",
        "        tokenizer_encode_args={\"return_tensors\": \"pt\"},\n",
        "        semeval_format=False,\n",
        "        gold_path=None,\n",
        "        transforms=[hypo_term_hyper],\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transforms = transforms\n",
        "        # сюда могут идти немного другие аргументы если допустим я использую Dolly а не T5\n",
        "        self.tokenizer_encode_args = tokenizer_encode_args\n",
        "        # в формате SemEval дебильные датасеты, мы их тут соединим\n",
        "        if semeval_format:\n",
        "            assert gold_path is not None\n",
        "            train_data_en_data = pd.read_csv(\n",
        "                data_path, header=None, sep=\"\\t\", names=[\"term\", \"relation\"]\n",
        "            )\n",
        "            train_gold_en_data = pd.read_csv(gold_path, header=None, names=[\"hypernym\"])\n",
        "\n",
        "            self.df = pd.concat([train_data_en_data, train_gold_en_data], axis=1)[\n",
        "                [\"term\", \"hypernym\"]\n",
        "            ]\n",
        "        # предположительно в нашем датасете уже все ок, но это опицональная часть\n",
        "        else:\n",
        "            self.df = pd.read_csv(\n",
        "                data_path, header=None, sep=\"\\t\", names=[\"term\", \"hypernym\"]\n",
        "            )\n",
        "\n",
        "        self.df.index = list(range(len(self.df)))\n",
        "\n",
        "    # в данном случае выход под LM модельку с маск токеном -100\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.loc[index]\n",
        "        term = row[\"term\"]\n",
        "        target = \", \".join(row[\"hypernym\"].split(\"\\t\"))\n",
        "\n",
        "        # заранее пишу более общо, чтобы мы могли разне процессинги пробовать, а в будущем рандомно выбирать и тд\n",
        "        # это типа мы подаем список трансформаций затравок\n",
        "        processed_term = self.transforms[0](term)\n",
        "\n",
        "        input_ids = processed_term + target\n",
        "\n",
        "        out = tokenizer.encode_plus(input_ids)\n",
        "\n",
        "        # токенизируем\n",
        "       # encoded_term = self.tokenizer.encode(\n",
        "        #    processed_term, **self.tokenizer_encode_args\n",
        "       # )\n",
        "       # encoded_target = self.tokenizer.encode(target, **self.tokenizer_encode_args)\n",
        "\n",
        "        #input_seq = torch.concat([encoded_term, encoded_target], dim=1)\n",
        "       # labels = input_seq.clone()\n",
        "       # labels[0, : encoded_term.size()[1]] = -100\n",
        "\n",
        "        return {\n",
        "           \"input_ids\": out['input_ids'],\n",
        "           \"attention_mask\": out['attention_mask']\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "class LanguageModelingDataCollator(DataCollatorForLanguageModeling):\n",
        "    def __call__(self, examples):\n",
        "        input_ids = [example['input_ids'] for example in examples]\n",
        "        attention_mask = [example['attention_mask'] for example in examples]\n",
        "        labels = [example['input_ids'] for example in examples]\n",
        "        max_length = max(len(ids) for ids in input_ids)\n",
        "        input_ids = torch.tensor([ids + [self.tokenizer.pad_token_id] * (max_length - len(ids)) for ids in input_ids])\n",
        "        attention_mask = torch.tensor([mask + [0] * (max_length - len(mask)) for mask in attention_mask])\n",
        "        labels = torch.tensor([ids + [-100] * (max_length - len(ids)) for ids in labels]) # -100 is the default value for ignored labels in CrossEntropyLoss\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b3675f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# data\n",
        "train_dataset = HypernymDataset(\n",
        "    data_path=config.data_path,\n",
        "    tokenizer=tokenizer,\n",
        "    gold_path=config.gold_path,\n",
        "    semeval_format=True,\n",
        ")\n",
        "test_dataset = HypernymDataset(\n",
        "    data_path=config.test_data_path,\n",
        "    tokenizer=tokenizer,\n",
        "    gold_path=config.test_gold_path,\n",
        "    semeval_format=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cacd3c0-68be-42f7-8096-ac5581afb449",
      "metadata": {
        "id": "6cacd3c0-68be-42f7-8096-ac5581afb449"
      },
      "outputs": [],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BtkZd3UPzOzz",
      "metadata": {
        "id": "BtkZd3UPzOzz"
      },
      "outputs": [],
      "source": [
        "collator = LanguageModelingDataCollator(tokenizer, mlm=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a815dc92",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    collate_fn=collator,\n",
        "    shuffle=True,\n",
        "    num_workers=8,\n",
        "    drop_last=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23766dfd",
      "metadata": {},
      "outputs": [],
      "source": [
        "for batch in train_loader:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6a4de89",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.batch_decode(batch['input_ids'], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "916b3385",
      "metadata": {},
      "outputs": [],
      "source": [
        "batch['labels'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d122912d",
      "metadata": {},
      "outputs": [],
      "source": [
        "batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q4Wbpua0LKZB",
      "metadata": {
        "id": "Q4Wbpua0LKZB"
      },
      "outputs": [],
      "source": [
        "from dataset.dataset import HypernymDataset, Collator\n",
        "from functools import partial\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DWfjQ-Ty50ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DWfjQ-Ty50ac",
        "outputId": "90296bcc-4fd4-41ad-e19e-0f8ab50408d7"
      },
      "outputs": [],
      "source": [
        "f\"{config.model_checkpoint}_epoch={epoch}_MAP={metrics['MAP']}.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XacrNoZLSCkD",
      "metadata": {
        "id": "XacrNoZLSCkD"
      },
      "outputs": [],
      "source": [
        "# path_data_en = \"data_subset.txt\"\n",
        "# path_gold_en = \"gold_subset.txt\"\n",
        "\n",
        "# path_test_data_en = \"test_subset.txt\"\n",
        "# path_test_gold_en = \"test_gold_subset.txt\"\n",
        "\n",
        "\n",
        "train_dataset = HypernymDataset(data_path = path_data_en, tokenizer=tokenizer, semeval_format=True, gold_path=path_gold_en)\n",
        "val_dataset = HypernymDataset(data_path = path_test_data_en, tokenizer=tokenizer, semeval_format=True, gold_path=path_test_gold_en)\n",
        "\n",
        "collator = Collator(tokenizer.eos_token_id, tokenizer.eos_token_id)\n",
        "\n",
        "BS = 16\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=BS,\n",
        "                                           collate_fn=collator)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                         batch_size=BS,\n",
        "                                         collate_fn=collator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_kykoFR-PVWl",
      "metadata": {
        "id": "_kykoFR-PVWl"
      },
      "outputs": [],
      "source": [
        "%autoreload 2\n",
        "\n",
        "from config.config import TaskConfig\n",
        "from train import CustomScheduler\n",
        "from logger.logger import WanDBWriter\n",
        "from trainer.train_epoch import train_epoch, predict\n",
        "from metrics.metrics import get_all_metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4M2x-ixULEcY",
      "metadata": {
        "id": "4M2x-ixULEcY"
      },
      "outputs": [],
      "source": [
        "config = TaskConfig()\n",
        "config.compute_metrics_every = len(train_loader) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k_ssXh2567zg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_ssXh2567zg",
        "outputId": "5c0165bd-ea43-42d1-8aaf-740513e2a528"
      },
      "outputs": [],
      "source": [
        "all_preds, all_labels = predict(model, tokenizer, val_loader, config)\n",
        "metrics = get_all_metrics(all_labels, all_preds)\n",
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PwEha0DK76Xv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "PwEha0DK76Xv",
        "outputId": "0be023cb-441b-4f5e-e493-dc5958088935"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "# optmizations\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, betas=(0.9, 0.98), eps=1e-9)\n",
        "scheduler = CustomScheduler(1024, optimizer, config.warmup)\n",
        "\n",
        "# # wandb\n",
        "logger = WanDBWriter(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w9iLJwep9Hx8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "w9iLJwep9Hx8",
        "outputId": "48c53936-cd1d-4581-f029-59c65cc7885c"
      },
      "outputs": [],
      "source": [
        "for epoch in range(config.n_epochs):\n",
        "    print(f\"Start of the epoch {epoch}\")\n",
        "    train_epoch(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        scheduler,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        criterion,\n",
        "        logger,\n",
        "        config,\n",
        "        epoch,\n",
        "    )\n",
        "\n",
        "\n",
        "    all_preds, all_labels = predict(model, tokenizer, val_loader, config)\n",
        "    metrics = get_all_metrics(all_labels, all_preds)\n",
        "    for key in metrics:\n",
        "        logger.add_scalar(key, float(metrics[key][0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jHIOaQgkATxW",
      "metadata": {
        "id": "jHIOaQgkATxW"
      },
      "outputs": [],
      "source": [
        "st = logger.get_step() + 1\n",
        "torch.save(\n",
        "    {\n",
        "        \"model\": model.state_dict(),\n",
        "        # 'opt': optimizer.state_dict(),\n",
        "        #\"sch\": scheduler.state_dict(),\n",
        "    },\n",
        "    f\"best_model_{st}_{epoch}.pth\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ppxm8lKzrYel",
      "metadata": {
        "id": "ppxm8lKzrYel"
      },
      "outputs": [],
      "source": [
        "config.gen_args = {'top_k': 50,\n",
        "                   'max_new_tokens': 25,\n",
        "                   \"temperature\": 0.2,\n",
        "                   \"do_sample\": True\n",
        "                   }\n",
        "\n",
        "config.gen_args = {'num_beams': 5,\n",
        "                   'max_new_tokens': 25,\n",
        "                   \"early_stopping\": True\n",
        "                   }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RUBp5i1Axtzn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUBp5i1Axtzn",
        "outputId": "5d5cf473-bb8c-4aad-fa29-f010d6de29ae"
      },
      "outputs": [],
      "source": [
        "variated_beams = [3,5,10,15]\n",
        "res = []\n",
        "\n",
        "for beam in variated_beams:\n",
        "    config.gen_args = {'num_beams': beam,\n",
        "                   'max_new_tokens': 25,\n",
        "                   \"early_stopping\": True\n",
        "                   }\n",
        "\n",
        "    all_preds, all_labels = predict(model, tokenizer, val_loader, config)\n",
        "    metrics = get_all_metrics(all_labels, all_preds)\n",
        "\n",
        "    res.append(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XDBznN2F065F",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "XDBznN2F065F",
        "outputId": "3c4c6211-66e4-4841-fa5a-6151bdbbf7e4"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LHcmacseqtT4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHcmacseqtT4",
        "outputId": "18eb9b92-5936-448b-8394-34354e471b38"
      },
      "outputs": [],
      "source": [
        "all_preds, all_labels = predict(model, tokenizer, val_loader, config)\n",
        "metrics = get_all_metrics(all_labels, all_preds)\n",
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yR4JmjqXHLJR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR4JmjqXHLJR",
        "outputId": "e4e36c21-2950-4ce3-fff2-99e6bd520f24"
      },
      "outputs": [],
      "source": [
        "list(zip(all_preds, all_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TJ5uEtDR981n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ5uEtDR981n",
        "outputId": "d10ba298-2aff-4ed4-e9ee-a066812e05c1"
      },
      "outputs": [],
      "source": [
        "type(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aYCF-jGECBSr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYCF-jGECBSr",
        "outputId": "16eb6ca0-249f-45c4-9027-f8cdccfac4c2"
      },
      "outputs": [],
      "source": [
        "type(metrics[key][0].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1A5aiiKy-ABO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A5aiiKy-ABO",
        "outputId": "e78ecd71-6c2e-404c-bb9c-6849863d481c"
      },
      "outputs": [],
      "source": [
        "input_seqs.requires_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BIHd9ld2FlBQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIHd9ld2FlBQ",
        "outputId": "112dff13-53ce-40df-c664-df28b0b54347"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode( train_dataset[0]['encoded_term']))\n",
        "print(tokenizer.decode( train_dataset[0]['encoded_target']))\n",
        "output = model.forward(\n",
        "    train_dataset[0]['input_seq'].to(config.device),\n",
        "    labels=train_dataset[0]['labels'].to(config.device)\n",
        "    )\n",
        "print(output['loss'].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cvXhpBluEA8S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvXhpBluEA8S",
        "outputId": "34744daa-9bac-438e-931a-a275a9641ae6"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode( train_dataset[1]['encoded_term']))\n",
        "print(tokenizer.decode( train_dataset[1]['encoded_target']))\n",
        "output = model.forward(\n",
        "    train_dataset[1]['input_seq'].to(config.device),\n",
        "    labels=train_dataset[1]['labels'].to(config.device)\n",
        ")\n",
        "print(output['loss'].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efPajX8vFvJp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efPajX8vFvJp",
        "outputId": "e5f09f3e-412d-4a28-9890-899532c20c5c"
      },
      "outputs": [],
      "source": [
        "tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b9c4baf-fcb8-4178-a31a-8bdcff0fbe41",
      "metadata": {
        "id": "0b9c4baf-fcb8-4178-a31a-8bdcff0fbe41"
      },
      "outputs": [],
      "source": [
        "from evaluation.data_collat import DataCollatorWithPadding\n",
        "from evaluation.experiment import Experiment"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f0809810-2abd-4182-8b77-80e843cda409",
      "metadata": {
        "id": "f0809810-2abd-4182-8b77-80e843cda409"
      },
      "source": [
        "1. train params/predict params [1, 1]\n",
        "2. train / params predict [0, 1]\n",
        "3. params predict [0, 0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7866ee63-7759-4fc2-beae-d345b2d18a9b",
      "metadata": {
        "id": "7866ee63-7759-4fc2-beae-d345b2d18a9b"
      },
      "source": [
        "## Change only parmeters in generating with no training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21a56839-766a-4911-bd14-b0fb79f71740",
      "metadata": {
        "id": "21a56839-766a-4911-bd14-b0fb79f71740"
      },
      "outputs": [],
      "source": [
        "check_param = {'GenArgs': {'num_beams': [2]}}\n",
        "\n",
        "freezed_param = {'TrainArgs': {'num_train_epochs':1, 'per_device_train_batch_size':16, 'save_steps':1}, \n",
        "                 'GenArgs': {'max_new_tokens': 3, 'early_stopping': True, \"pad_token_id\": tokenizer.eos_token_id}, \n",
        "                 'SelectStrategy': None, 'PredForm': None}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wsNn9JDMPIYj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsNn9JDMPIYj",
        "outputId": "9d1fe9c9-cf99-4673-836a-0dfcd35d1ca9"
      },
      "outputs": [],
      "source": [
        "!mkdir output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78419336-4edb-47a8-b261-96661c48fb59",
      "metadata": {
        "id": "78419336-4edb-47a8-b261-96661c48fb59"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "path_data = '/content/NLP-DL-Project-hypo-to-hyper/SemEval2018-Task9/test/gold/1A.english.test.gold.txt'\n",
        "\n",
        "experiment1 = Experiment(output_dir='output/',\n",
        "                         check_param=check_param, freezed_param=freezed_param,\n",
        "                         model=model, tokenizer=tokenizer, device=device,\n",
        "                         data_collator=data_collator,\n",
        "                         data_train=train_data_en.tolist(), target_train=train_gold_en.tolist(), \n",
        "                         data_test=test_data_en.tolist(), target_test=test_gold_en.tolist(), strategy=[0, 0], \n",
        "                         path_to_test=path_data\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AKqpiQ-LC_p-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKqpiQ-LC_p-",
        "outputId": "a1997fb2-9604-4f9b-ee95-380ce3ef4bc1"
      },
      "outputs": [],
      "source": [
        "test_data_en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nbNumJ2JqPc1",
      "metadata": {
        "id": "nbNumJ2JqPc1"
      },
      "outputs": [],
      "source": [
        "for text in test_data_en.tolist(): \n",
        "    out = tokenizer.encode(text, return_tensors='pt')  \n",
        "    #input_ids = torch.concat([input_ids, torch.zeros((1, 1), dtype=torch.int)], dim=1)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yM18I3wj-KWE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM18I3wj-KWE",
        "outputId": "6dc64e49-defc-41ee-f46c-18d36cb78813"
      },
      "outputs": [],
      "source": [
        "tokenizer(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5J3fqUOiqeFm",
      "metadata": {
        "id": "5J3fqUOiqeFm"
      },
      "outputs": [],
      "source": [
        "outputs = model.generate(input_ids.to(device),\n",
        "                         num_beams=2,\n",
        "                         max_new_tokens=3,\n",
        "                         pad_token_id=tokenizer.eos_token_id) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jx1k3dBv8HoU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jx1k3dBv8HoU",
        "outputId": "a56878bc-521c-4639-bad3-938b5194d895"
      },
      "outputs": [],
      "source": [
        "input_ids, outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BKzVbbo_AqtV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKzVbbo_AqtV",
        "outputId": "6ff13068-027c-48cb-b751-a2d54e6249cd"
      },
      "outputs": [],
      "source": [
        "input_ids.size()[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rnVJRFLhAh-5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnVJRFLhAh-5",
        "outputId": "d25400ca-adbd-4c21-edba-710c2bfccbd1"
      },
      "outputs": [],
      "source": [
        "outputs[:,input_ids.size()[1]:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iBkEsUjYrdVs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iBkEsUjYrdVs",
        "outputId": "cc398303-295e-46c9-a848-13dbff4ef1b1"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(outputs[:,input_ids.size()[1]:][0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43384857-c13a-4330-8f32-422ca0fe9a54",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "43384857-c13a-4330-8f32-422ca0fe9a54",
        "outputId": "e270639e-6b1f-42aa-f9b9-3e83e998dbe3",
        "tags": []
      },
      "outputs": [],
      "source": [
        "experiment1.run_experiment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tMmYoN2fPaRP",
      "metadata": {
        "id": "tMmYoN2fPaRP"
      },
      "outputs": [],
      "source": [
        "experiment1.current_model.tokenize(\n",
        "            experiment1.data_train, experiment1.target_train,\n",
        "             experiment1.data_test, experiment1.target_test\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OfyQNyS6QUh9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfyQNyS6QUh9",
        "outputId": "466bd571-1e45-4be4-bc6c-9d8ee2487ea9"
      },
      "outputs": [],
      "source": [
        "experiment1.current_model.TrainDataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cWOJzNVr0egC",
      "metadata": {
        "id": "cWOJzNVr0egC"
      },
      "outputs": [],
      "source": [
        "_std_out = subprocess.check_output(['python',\n",
        "                                    os.path.join(os.getcwd(), 'evaluation/custom_scorer.py'), \n",
        "                                    path_data,\n",
        "                                    '/content/NLP-DL-Project-hypo-to-hyper/experiment/0/experiment.txt'])\n",
        "_std_out = _std_out.decode('UTF-8')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "seLvq6rn1n4e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "seLvq6rn1n4e",
        "outputId": "a31931bd-0c83-416d-f494-9d7c8d81bc6a"
      },
      "outputs": [],
      "source": [
        "_std_out"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "bd961bff-cd17-437d-97c2-4463c17038b5",
      "metadata": {
        "id": "bd961bff-cd17-437d-97c2-4463c17038b5"
      },
      "source": [
        "## Change only parmeters in generating with training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DcAQWuOjw_Gt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcAQWuOjw_Gt",
        "outputId": "8599b3df-e153-40f9-bafb-d3d9c7e9b9f3"
      },
      "outputs": [],
      "source": [
        "_std_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c28c5062-429e-4965-bc3c-876201e9734c",
      "metadata": {
        "id": "c28c5062-429e-4965-bc3c-876201e9734c"
      },
      "outputs": [],
      "source": [
        "check_param = {'GenArgs': {'num_beams': [2, 10]}}\n",
        "\n",
        "freezed_param = {'TrainArgs': {'num_train_epochs':1, 'per_device_train_batch_size':16, 'save_steps':1}, \n",
        "                 'GenArgs': {'max_length': 3, 'early_stopping': True}, \n",
        "                 'SelectStrategy': None, 'PredForm': None}\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "path_data = '/content/NLP-DL-Project-hypo-to-hyper/SemEval2018-Task9/test/gold/1A.english.test.gold.txt'\n",
        "\n",
        "experiment1 = Experiment(output_dir='output/',\n",
        "                         check_param=check_param, freezed_param=freezed_param,\n",
        "                         model=model, tokenizer=tokenizer, device=device,\n",
        "                         data_collator=data_collator,\n",
        "                         data_train=train_data_en.tolist(), target_train=train_gold_en.tolist(), \n",
        "                         data_test=test_data_en.tolist(), target_test=test_gold_en.tolist(),\n",
        "                         strategy=[0, 1], \n",
        "                         path_to_test=path_data\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "800bf82a-bc02-461d-97d0-0cd46887356a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "800bf82a-bc02-461d-97d0-0cd46887356a",
        "outputId": "f6788311-259c-4ab3-b4a2-b636ee5f5817"
      },
      "outputs": [],
      "source": [
        "experiment1.run_experiment()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1b90423c-2dcb-477d-a484-230949f020c8",
      "metadata": {
        "id": "1b90423c-2dcb-477d-a484-230949f020c8"
      },
      "source": [
        "## Change generating and training parmeters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f16d5ee-cb5e-43e2-8399-1fd55e1d482e",
      "metadata": {
        "id": "2f16d5ee-cb5e-43e2-8399-1fd55e1d482e"
      },
      "outputs": [],
      "source": [
        "check_param = {'TrainArgs': {'num_train_epochs':[1, 2]}, 'GenArgs': {'num_beams': [2, 10]}}\n",
        "\n",
        "freezed_param = {'TrainArgs': {'per_device_train_batch_size':16, 'save_steps':1}, \n",
        "                 'GenArgs': {'max_length': 3, 'early_stopping': True}, \n",
        "                 'SelectStrategy': None, 'PredForm': None}\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "path_data = '/content/NLP-DL-Project-hypo-to-hyper/SemEval2018-Task9/test/gold/1A.english.test.gold.txt'\n",
        "\n",
        "experiment1 = Experiment(output_dir='/home/jovyan/work',\n",
        "                         check_param=check_param, freezed_param=freezed_param,\n",
        "                         model=model, tokenizer=tokenizer, device=device,\n",
        "                         data_collator=data_collator,\n",
        "                         data_train=train_data_en.tolist(), target_train=train_gold_en.tolist(), \n",
        "                         data_test=test_data_en.tolist(), target_test=test_gold_en.tolist(),\n",
        "                         strategy=[1, 1], \n",
        "                         path_to_test=path_data\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50e7c844-df40-423a-b0aa-3818c103c939",
      "metadata": {
        "id": "50e7c844-df40-423a-b0aa-3818c103c939",
        "outputId": "9d394b9c-1c48-4f8a-d8a5-37dafb556aea",
        "tags": []
      },
      "outputs": [],
      "source": [
        "experiment1.run_experiment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8265237-ddf6-46f5-9252-97e4eb281705",
      "metadata": {
        "id": "b8265237-ddf6-46f5-9252-97e4eb281705",
        "outputId": "720b5dd5-986a-4bef-dc42-bc2e8a0e2aaa"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66d4ce72-20cf-438a-9be1-47f77ab48556",
      "metadata": {
        "id": "66d4ce72-20cf-438a-9be1-47f77ab48556",
        "outputId": "195951dd-d2fb-4fae-c24e-f014c2cf9c39"
      },
      "outputs": [],
      "source": [
        "!rm -rf experiment"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0369c69255434eb5920af69c6b0eab30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a66592bb7f84d43ac0261d1306b908b",
            "placeholder": "​",
            "style": "IPY_MODEL_5942d744160e4705b50f48617fb19c80",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "0e4c95c75e1e4756826b09c5afad0fb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0eaa66a2acc34675863228355a166f7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cfa5bea3cae74696a01c37a0b5726509",
              "IPY_MODEL_90e2a4749d2a438ea396ac2a60fb40f7",
              "IPY_MODEL_487c84226af5476d8a2a32b0f79acacd"
            ],
            "layout": "IPY_MODEL_5df817497b5547cf951e1c318eeb9535"
          }
        },
        "1512545756a84b759c02c85eff980fd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3426456a50134da589ba163fdd079ce0",
            "placeholder": "​",
            "style": "IPY_MODEL_3fc22325c69e4292a3d2fd002ae4c471",
            "value": " 1.01k/1.01k [00:00&lt;00:00, 64.1kB/s]"
          }
        },
        "17278cd2d15f49c1b01d9b5863158778": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96062273d13f423f930bdc02eb45a6c8",
            "placeholder": "​",
            "style": "IPY_MODEL_282e1bb49fd849ae91d40fa79bffdb6f",
            "value": " 899k/899k [00:00&lt;00:00, 13.1MB/s]"
          }
        },
        "19227bef944244a2999a2930a9238146": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2188d5d7d9b540aba4609e8c4b756fdb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "229a208f05fa4168b07c3bb3ce522dcb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "282e1bb49fd849ae91d40fa79bffdb6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b5300afd4544140aa6119c705322c69": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2eca7409d1b84c178fe6d3aada1dc9f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32a03fd30cee4b67af5bd7a1729ae96f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf8936b2f9b94452bfcd2b66ad6b405b",
              "IPY_MODEL_8cf6f3dc53d74da7813fb8cd8924438e",
              "IPY_MODEL_1512545756a84b759c02c85eff980fd1"
            ],
            "layout": "IPY_MODEL_9a2bc584900942b8aa8a35b70ab463e5"
          }
        },
        "3426456a50134da589ba163fdd079ce0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34802e62dc4c4887af3b9396b7ebb737": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35e2bddbdffe41939a9dcd88dc965e92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19227bef944244a2999a2930a9238146",
            "placeholder": "​",
            "style": "IPY_MODEL_3a9f1743e44e4697946ea275a8e25c56",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "3662ab3f3c694063b4543a59dfac236e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a9f1743e44e4697946ea275a8e25c56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fc22325c69e4292a3d2fd002ae4c471": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "437bff5f3034482a91c7dcf3f3bc9d92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7dfeda09dd9420db74e6b5ab63f02fc",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_635ec7b3dc574637aaca674f976b11ad",
            "value": 456318
          }
        },
        "487c84226af5476d8a2a32b0f79acacd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5072d9eda7e24331874f02f5951141e4",
            "placeholder": "​",
            "style": "IPY_MODEL_a49a8799b49344bdabf11b20657cfa05",
            "value": " 526M/526M [00:05&lt;00:00, 102MB/s]"
          }
        },
        "4acb74c5858f44a3ad5126b158cc9e47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4de9408cd05949a989fd9f24dc2714c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5072d9eda7e24331874f02f5951141e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "516ffbad03ae4cafac46641cb399aa78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2d361d0ba3649d59f6d1c5a3f2af985",
            "placeholder": "​",
            "style": "IPY_MODEL_a1949c2fcd774558b90ec10efbd6d203",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "5726d739909d433fae883a3eadcb8337": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5942d744160e4705b50f48617fb19c80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d1f3cb411f84ea6a87ecec032e4c4e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dba750a5b5c42b4b9f1d34683347660": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2188d5d7d9b540aba4609e8c4b756fdb",
            "placeholder": "​",
            "style": "IPY_MODEL_3662ab3f3c694063b4543a59dfac236e",
            "value": " 456k/456k [00:00&lt;00:00, 25.5MB/s]"
          }
        },
        "5df817497b5547cf951e1c318eeb9535": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view