{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1741\n",
      "1610\n"
     ]
    }
   ],
   "source": [
    "# names = ['../triplets_sci/lemmas_triplets.pickle',\n",
    "#          '../triplets_sci/numbers_triplets.pickle',\n",
    "#           ]\n",
    "\n",
    "names = ['../triplets_env/lemmas_c_triplets.pickle',\n",
    "         '../triplets_env/numbers_c_triplets.pickle',\n",
    "          ]\n",
    "\n",
    "total = []\n",
    "for name in names:\n",
    "    with open(name, 'rb') as f:\n",
    "        ls = pickle.load(f)\n",
    "        print(len(ls))\n",
    "        total += ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '../immutual_env_lemmas.pickle'\n",
    "with open(name, 'rb') as f:\n",
    "    ls = pickle.load(f)\n",
    "\n",
    "path_env = '../gs_taxo/EN/environment_eurovoc_en.taxo'\n",
    "G_env = nx.DiGraph()\n",
    "\n",
    "with open(path_env, 'r') as f:\n",
    "    for line in f:\n",
    "        idx, hypo, hyper = line.split('\\t')\n",
    "        hyper = hyper.replace('\\n', '')\n",
    "        G_env.add_node(hypo)\n",
    "        G_env.add_node(hyper)\n",
    "        G_env.add_edge(hyper, hypo)\n",
    "\n",
    "path_sci = '../gs_taxo/EN/environment_eurovoc_en.taxo'\n",
    "G_sci = nx.DiGraph()\n",
    "\n",
    "with open(path_sci, 'r') as f:\n",
    "    for line in f:\n",
    "        idx, hypo, hyper = line.split('\\t')\n",
    "        hyper = hyper.replace('\\n', '')\n",
    "        G_sci.add_node(hypo)\n",
    "        G_sci.add_node(hyper)\n",
    "        G_sci.add_edge(hyper, hypo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_all_insertions(hyper, hypo, G):\n",
    "    temp_ls = []\n",
    "    for node in G.nodes():\n",
    "        if (node != hyper) and (node != hypo) and (node != 'environment') and (node != 'science'):\n",
    "            temp_ls.append((hyper, node, hypo))\n",
    "\n",
    "    return temp_ls\n",
    "\n",
    "def fill_ls(ls, G):\n",
    "\n",
    "    cur_temp = []\n",
    "\n",
    "    for child, parent in ls:\n",
    "        cur_temp.extend(make_all_insertions(hyper=parent, hypo=child, G=G))\n",
    "\n",
    "    return cur_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_conflicts = fill_ls(ls, G_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_insertions = list(set(env_conflicts + total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_insertions(total_ls):\n",
    "    # transformed_term = (\n",
    "    #         \"hyperhypenym: \"\n",
    "    #         + clean[\"grandparents\"]\n",
    "    #         + \" (\"\n",
    "    #         + elem[\"grandparent_def\"]\n",
    "    #         + \"), hyponym: \"\n",
    "    #         + clean[\"children\"]\n",
    "    #         + \" (\"\n",
    "    #         + elem[\"child_def\"]\n",
    "    #         + \") | hypernym:\"\n",
    "    #     )\n",
    "    # return transformed_term, clean[\"parents\"] + \",\"\n",
    "\n",
    "    ls = []\n",
    "    for triplet in set(total_ls):\n",
    "        elem = {}\n",
    "        elem[\"children\"] = triplet[2]\n",
    "        elem[\"parents\"] = triplet[1]\n",
    "        elem[\"grandparents\"] = triplet[0]\n",
    "        elem[\"case\"] = \"simple_triplet_grandparent\"\n",
    "        elem[\"grandparent_def\"] = triplet[0]\n",
    "        elem[\"child_def\"] = triplet[2]\n",
    "    \n",
    "        ls.append(elem)\n",
    "\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "insertions_env = collect_insertions(total_insertions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = '../data/env/eurovoc/raw_pairs/insertions_lemmas.pickle'\n",
    "with open(out_path, 'wb') as f:\n",
    "    pickle.dump(insertions_env, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_mix_nodes(total_ls, out_path):\n",
    "\n",
    "    ls = []\n",
    "    for triplet in set(total_ls):\n",
    "        elem = {}\n",
    "        elem[\"children\"] = triplet[2]\n",
    "        elem[\"parents\"] = [triplet[0], triplet[1]]\n",
    "        elem[\"grandparents\"] = None\n",
    "        elem[\"case\"] = \"simple_triplet_2parent\"\n",
    "        elem[\"1parent_def\"] = triplet[0]\n",
    "        elem[\"2parent_def\"] = triplet[1]\n",
    "    \n",
    "        ls.append(elem)\n",
    "\n",
    "    with open(out_path, 'wb') as f:\n",
    "        pickle.dump(ls, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_mix_nodes(total, '../data/env/eurovoc/raw_pairs/mix_nodes_lemmas.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/env/eurovoc/raw_pairs/mix_nodes_lemmas.pickle', 'rb') as f:\n",
    "    ls = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31066"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "for triplet in set(total):\n",
    "    elem = {}\n",
    "    elem[\"children\"] = triplet[2]\n",
    "    elem[\"parents\"] = [triplet[0], triplet[1]]\n",
    "    elem[\"grandparents\"] = None\n",
    "    elem[\"case\"] = \"simple_triplet_2parent\"\n",
    "    elem[\"1parent_def\"] = triplet[0]\n",
    "    elem[\"2parent_def\"] = triplet[1]\n",
    "\n",
    "ls.append(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'children': 'industrial sociology',\n",
       " 'parents': ['industrial chemistry', 'ethnology'],\n",
       " 'grandparents': None,\n",
       " 'case': 'simple_triplet_2parent',\n",
       " '1parent_def': 'industrial chemistry',\n",
       " '2parent_def': 'ethnology'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_elem(elem, keys_to_remove_digits=[\"children\"]):\n",
    "    removes = set(keys_to_remove_digits)\n",
    "    if not \"changed\" in elem.keys():\n",
    "        for field in [\"children\", \"parents\", \"grandparents\", \"brothers\"]:\n",
    "            if field in elem.keys():\n",
    "                elem[field] = delete_techniqal(\n",
    "                    elem[field], remove=True\n",
    "                )  # (field in removes))\n",
    "                elem[\"changed\"] = True\n",
    "    return elem\n",
    "\n",
    "\n",
    "def delete_techniqal(elem, remove):\n",
    "    if isinstance(elem, str):\n",
    "        if \".n.\" in elem and remove:\n",
    "            return elem.split(\".\")[0].replace(\"_\", \" \")\n",
    "        else:\n",
    "            return elem.replace(\"_\", \" \")\n",
    "\n",
    "    elif isinstance(elem, list):\n",
    "        new_words = []\n",
    "        for word in elem:\n",
    "            new_words.append(delete_techniqal(word, remove))\n",
    "        return new_words\n",
    "elem = insertions_env[0]\n",
    "clean = clean_elem(elem, keys_to_remove_digits=[\"children\"])\n",
    "transformed_term = (\n",
    "        \"hyperhypenym: \"\n",
    "        + clean[\"grandparents\"]\n",
    "        + \" (\"\n",
    "        + elem[\"grandparent_def\"]\n",
    "        + \"), hyponym: \"\n",
    "        + clean[\"children\"]\n",
    "        + \" (\"\n",
    "        + elem[\"child_def\"]\n",
    "        + \") | hypernym:\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hyperhypenym: hazardous waste (hazardous waste), hyponym: pollution of waterways (pollution of waterways) | hypernym:'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperhypenym: hazardous waste (hazardous waste), hyponym: pollution of waterways (pollution of waterways) | hypernym:\n"
     ]
    }
   ],
   "source": [
    "transformed_term\n",
    "print(transformed_term)\n",
    "s = transformed_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hazardous waste'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term = s.split(\"|\")[-2]\n",
    "term = term.split(\":\")\n",
    "term[1].split(' (')[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pollution of waterways'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term[2].split('(')[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term(s, case):\n",
    "    term = s.split(\"|\")[-2]\n",
    "    term = term.split(\":\")\n",
    "    if (case == 'pred_hypernym') or (case == 'leaf_no_leafs'):\n",
    "        return term[-1].strip()\n",
    "    elif (case == 'simple_triplet_2parent') or (case == 'simple_triplet_grandparent'):\n",
    "        first_parent = term[1].split(' (')[0].strip()\n",
    "        second_parent = term[2].split('(')[0].strip()\n",
    "        \n",
    "        # in case of grandparent, it would be hyperhypernym_hyponym\n",
    "        return first_parent + '_' + second_parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hazardous waste_pollution of waterways'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_term(transformed_term, case=elem['case'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ' industrial chemistry (industrial chemistry), second hypernym'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' industrial chemistry ', 'industrial chemistry), second hypernym']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split('(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/env/eurovoc/raw_pairs/pred_hypernym_lemmas.pickle', 'rb') as f:\n",
    "    ph = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'children': 'climate change policy',\n",
       " 'parents': 'adaptation to climate change',\n",
       " 'grandparents': None,\n",
       " 'case': 'predict_hypernym',\n",
       " 'child_def': 'climate change policy'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ph[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = clean_elem(ph[0], keys_to_remove_digits=[\"parents\"])\n",
    "\n",
    "transformed_term = (\n",
    "    \"hyponym: \" + clean[\"children\"] + \" (\" + ph[0][\"child_def\"] + \") | hypernym:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'climate change policy (climate change policy)'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_term(transformed_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
