{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from leafer import Leafer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "for synset in (wn.all_synsets('n')):\n",
    "    name = synset.name()\n",
    "    G.add_node(name)\n",
    "    hyponyms = synset.hyponyms()\n",
    "\n",
    "    for hypo in hyponyms:\n",
    "        new_name = hypo.name()\n",
    "        G.add_node(new_name)\n",
    "        G.add_edge(name, new_name)\n",
    "\n",
    "for synset in (wn.all_synsets('v')):\n",
    "    name = synset.name()\n",
    "    G.add_node(name)\n",
    "    hyponyms = synset.hyponyms()\n",
    "\n",
    "    for hypo in hyponyms:\n",
    "        new_name = hypo.name()\n",
    "        G.add_node(new_name)\n",
    "        G.add_edge(name, new_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95882"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun_path = '../../TaxonomyEnrichment/data/noun/train.edgelist'\n",
    "# verb_path = '../../TaxonomyEnrichment/data/verb/train.edgelist'\n",
    "\n",
    "# noun = nx.read_edgelist(noun_path, create_using=nx.DiGraph, delimiter='\\t')\n",
    "# verb = nx.read_edgelist(verb_path, create_using=nx.DiGraph, delimiter='\\t')\n",
    "\n",
    "# G = nx.compose(noun, verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude test\n",
    "\n",
    "# from MAGS\n",
    "cs_test_path = '../../TaxonomyEnrichment/data/MAG_CS/test_nodes.pickle'\n",
    "psy_test_path = '../../TaxonomyEnrichment/data/psychology/test_nodes.pickle'\n",
    "noun_test_path = '../../TaxonomyEnrichment/data/noun/test_nodes.pickle'\n",
    "verb_test_path = '../../TaxonomyEnrichment/data/verb/test_nodes.pickle'\n",
    "\n",
    "with open(cs_test_path, 'rb') as f:\n",
    "    cs_test = pickle.load(f)\n",
    "\n",
    "with open(psy_test_path, 'rb') as f:\n",
    "    psy_test = pickle.load(f)\n",
    "\n",
    "with open(noun_test_path, 'rb') as f:\n",
    "    noun_test = pickle.load(f)\n",
    "\n",
    "with open(verb_test_path, 'rb') as f:\n",
    "    verb_test = pickle.load(f)\n",
    "\n",
    "k = 0\n",
    "for node, parents in cs_test:\n",
    "    for i in range(10):\n",
    "        true_name = f'{node}.n.0{i}'\n",
    "        if true_name in G.nodes():\n",
    "            G.remove_node(true_name)\n",
    "            k += 1\n",
    "    \n",
    "for node, parents in psy_test:\n",
    "    for i in range(10):\n",
    "        true_name = f'{node}.n.0{i}'\n",
    "        if true_name in G.nodes():\n",
    "            G.remove_node(true_name)\n",
    "            k += 1\n",
    "\n",
    "for node, parents in verb_test:\n",
    "    if node in G.nodes():\n",
    "        G.remove_node(node)\n",
    "        k += 1\n",
    "\n",
    "for node, parents in noun_test:\n",
    "    if node in G.nodes():\n",
    "        G.remove_node(node)\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2096"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Hypernym Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = '../../SemEval2018-Task9/custom_datasets/1A.english.pickle'\n",
    "medical_path = '../../SemEval2018-Task9/custom_datasets/2A.medical.pickle'\n",
    "music_path = '../../SemEval2018-Task9/custom_datasets/2B.music.pickle'\n",
    "\n",
    "with open(main_path, 'rb') as f:\n",
    "    main = pickle.load(f)\n",
    "\n",
    "with open(medical_path, 'rb') as f:\n",
    "    medical = pickle.load(f)\n",
    "\n",
    "with open(music_path, 'rb') as f:\n",
    "    music = pickle.load(f)\n",
    "\n",
    "\n",
    "for elem in main:\n",
    "    node = elem['children'].replace(' ', '_')\n",
    "    for i in range(10):\n",
    "        true_name = f'{node}.n.0{i}'\n",
    "        if true_name in G.nodes():\n",
    "            G.remove_node(true_name)\n",
    "            k += 1\n",
    "\n",
    "for elem in medical:\n",
    "    node = elem['children'].replace(' ', '_')\n",
    "    for i in range(10):\n",
    "        true_name = f'{node}.n.0{i}'\n",
    "        if true_name in G.nodes():\n",
    "            G.remove_node(true_name)\n",
    "            k += 1\n",
    "\n",
    "for elem in music:\n",
    "    node = elem['children'].replace(' ', '_')\n",
    "    for i in range(10):\n",
    "        true_name = f'{node}.n.0{i}'\n",
    "        if true_name in G.nodes():\n",
    "            G.remove_node(true_name)\n",
    "            k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4257"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From TEXEVAL\n",
    "data = 'environment'\n",
    "env_path = \"../../TExEval-2_testdata_1.2/gs_taxo/EN/\" + str(data) + \"_eurovoc_en.taxo\"\n",
    "sci_path =  \"../../TExEval-2_testdata_1.2/gs_taxo/EN/\" + 'science' + \"_eurovoc_en.taxo\"\n",
    "G_test = nx.DiGraph()\n",
    "\n",
    "with open(env_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        idx, hypo, hyper = line.split(\"\\t\")\n",
    "        hyper = hyper.replace(\"\\n\", \"\")\n",
    "        G_test.add_node(hypo)\n",
    "        G_test.add_node(hyper)\n",
    "        G_test.add_edge(hyper, hypo)\n",
    "\n",
    "for node in G_test.nodes():\n",
    "    for i in range(10):\n",
    "        true_name = f'{node}.n.0{i}'\n",
    "        if true_name in G.nodes():\n",
    "            G.remove_node(true_name)\n",
    "            k += 1\n",
    "\n",
    "with open(sci_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        idx, hypo, hyper = line.split(\"\\t\")\n",
    "        hyper = hyper.replace(\"\\n\", \"\")\n",
    "        G_test.add_node(hypo)\n",
    "        G_test.add_node(hyper)\n",
    "        G_test.add_edge(hyper, hypo)\n",
    "\n",
    "for node in G_test.nodes():\n",
    "    for i in range(10):\n",
    "        true_name = f'{node}.n.0{i}'\n",
    "        if true_name in G.nodes():\n",
    "            G.remove_node(true_name)\n",
    "            k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('restrain.v.01', 'inhibit.v.04'), ('inhibit.v.04', 'restrain.v.01')]\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        cycle = nx.find_cycle(G)\n",
    "        print(cycle)\n",
    "        G.remove_edge(*cycle[0])\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_hypernym 36775 36775\n",
      "predict_hypernym 31 31\n"
     ]
    }
   ],
   "source": [
    "l = Leafer(G)\n",
    "\n",
    "\n",
    "train, test = l.split_train_test(\n",
    "    generation_depth=0,  # до какого уровня в топ. сортировке идти\n",
    "    p=0.001,  # вероятность что подходящий случай уйдет в тест\n",
    "    p_divide_leafs=0.5,\n",
    "    # вероятность что листья поделим пополам трейн-тест\n",
    "    # а не засунем целый случай в трейн или в тест\n",
    "    min_to_test_rate=0.5,\n",
    "    # минимальное количество доли вершин которых не было в\n",
    "    # трейне чтобы поделить пополам на трейн-тест\n",
    "    # то есть если 6\\10 вершин были трейне то значит все 10 в трейн\n",
    "    # если 5\\10 были в трейне, то значит оставшиеся можем кинуть в тест\n",
    "    weights=[0.00, 0.0, 0.0, 0.00, 0.00, 1.],\n",
    "    # веса в соответствии\n",
    "    # один ребенок, только листья, не только листья\n",
    "    # триплеты с 2 родителями, триплеты такие что мидл нода имеет\n",
    "    # 1 ребенка, предсказание родителя\n",
    "    #p_parent=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'children': 'window_seat.n.01',\n",
       "   'parents': 'bench.n.01',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'},\n",
       "  {'children': 'pummel.v.01',\n",
       "   'parents': 'hit.v.03',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'},\n",
       "  {'children': 'spanish_oak.n.01',\n",
       "   'parents': 'oak.n.02',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'}],\n",
       " [{'children': 'singular_matrix.n.01',\n",
       "   'parents': 'square_matrix.n.01',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'},\n",
       "  {'children': 'powdered_sugar.n.01',\n",
       "   'parents': 'granulated_sugar.n.01',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'},\n",
       "  {'children': 'general_anesthesia.n.01',\n",
       "   'parents': 'anesthesia.n.01',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'}])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:3], test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_definitions(elem):\n",
    "    if elem['case'] == 'predict_hypernym':\n",
    "        elem['child_def'] = wn.synset(elem['children'].replace(' ', '_')).definition()\n",
    "      #  elem['parent_def'] = wn.synset(elem['parents']).definition()\n",
    "    elif elem['case'] == 'predict_multiple_hypernyms':\n",
    "        elem['child_def'] = wn.synset(elem['children'].replace(' ', '_')).definition()\n",
    "\n",
    "    elif elem['case'] == 'simple_triplet_grandparent':\n",
    "        elem['child_def'] = wn.synset(elem['children'].replace(' ', '_')).definition()\n",
    "        elem['grandparent_def'] = wn.synset(elem['grandparents']).definition()\n",
    "    elif elem['case'] == 'only_child_leaf':\n",
    "        elem['grandparent_def'] = wn.synset(elem['grandparents']).definition()\n",
    "        elem['parent_def'] = wn.synset(elem['parents']).definition()\n",
    "    elif elem['case'] == 'simple_triplet_2parent':\n",
    "        elem['1parent_def'] = wn.synset(elem['parents'][0]).definition()\n",
    "        elem['2parent_def'] = wn.synset(elem['parents'][1]).definition()\n",
    "    else:\n",
    "        elem['parent_def'] = wn.synset(elem['parents']).definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i, elem in enumerate(train):\n",
    "    try:\n",
    "        add_definitions(elem)\n",
    "    except:\n",
    "        print(i, elem)\n",
    "        train.remove(elem)\n",
    "\n",
    "counter = 0\n",
    "for i, elem in enumerate(test):\n",
    "    try:\n",
    "        add_definitions(elem)\n",
    "    except:\n",
    "        print(i, elem)\n",
    "        counter += 1\n",
    "        test.remove(elem)\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out = '/home/LLM_Taxonomy/wnet/unified_clean_wnet_noun_verb_def_train.pickle'\n",
    "test_out = '/home/LLM_Taxonomy/wnet/unified_clean_wnet_noun_verb_def_test.pickle'\n",
    "\n",
    "with open(train_out, 'wb') as f:\n",
    "    pickle.dump(train, f)\n",
    "\n",
    "with open(test_out, 'wb') as f:\n",
    "    pickle.dump(test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NON SELECTIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_path = '../../TaxonomyEnrichment/data/noun/all.edgelist'\n",
    "verb_path = '../../TaxonomyEnrichment/data/verb/all.edgelist'\n",
    "\n",
    "noun = nx.read_edgelist(noun_path, create_using=nx.DiGraph, delimiter='\\t')\n",
    "verb = nx.read_edgelist(verb_path, create_using=nx.DiGraph, delimiter='\\t')\n",
    "\n",
    "G = nx.compose(noun, verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Leafer(G)\n",
    "# iterator = l.leafs_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_hypernym 44772 44772\n",
      "predict_hypernym 49 49\n"
     ]
    }
   ],
   "source": [
    "train, test = l.split_train_test(\n",
    "    generation_depth=0,  # до какого уровня в топ. сортировке идти\n",
    "    p=0.001,  # вероятность что подходящий случай уйдет в тест\n",
    "    p_divide_leafs=0.5,\n",
    "    # вероятность что листья поделим пополам трейн-тест\n",
    "    # а не засунем целый случай в трейн или в тест\n",
    "    min_to_test_rate=0.5,\n",
    "    # минимальное количество доли вершин которых не было в\n",
    "    # трейне чтобы поделить пополам на трейн-тест\n",
    "    # то есть если 6\\10 вершин были трейне то значит все 10 в трейн\n",
    "    # если 5\\10 были в трейне, то значит оставшиеся можем кинуть в тест\n",
    "    weights=[0.00, 0.0, 0.0, 0.00, 0.00, 1.],\n",
    "    # веса в соответствии\n",
    "    # один ребенок, только листья, не только листья\n",
    "    # триплеты с 2 родителями, триплеты такие что мидл нода имеет\n",
    "    # 1 ребенка, предсказание родителя\n",
    "    #p_parent=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'children': 'window_seat.n.01',\n",
       "   'parents': 'bench.n.01',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'},\n",
       "  {'children': 'pummel.v.01',\n",
       "   'parents': 'hit.v.03',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'},\n",
       "  {'children': 'spanish_oak.n.01',\n",
       "   'parents': 'oak.n.02',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'}],\n",
       " [{'children': 'singular_matrix.n.01',\n",
       "   'parents': 'square_matrix.n.01',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'},\n",
       "  {'children': 'powdered_sugar.n.01',\n",
       "   'parents': 'granulated_sugar.n.01',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'},\n",
       "  {'children': 'general_anesthesia.n.01',\n",
       "   'parents': 'anesthesia.n.01',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'}])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:3], test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_definitions(elem):\n",
    "    if elem['case'] == 'predict_hypernym':\n",
    "        elem['child_def'] = wn.synset(elem['children'].replace(' ', '_')).definition()\n",
    "      #  elem['parent_def'] = wn.synset(elem['parents']).definition()\n",
    "    elif elem['case'] == 'predict_multiple_hypernyms':\n",
    "        elem['child_def'] = wn.synset(elem['children'].replace(' ', '_')).definition()\n",
    "\n",
    "    elif elem['case'] == 'simple_triplet_grandparent':\n",
    "        elem['child_def'] = wn.synset(elem['children'].replace(' ', '_')).definition()\n",
    "        elem['grandparent_def'] = wn.synset(elem['grandparents']).definition()\n",
    "    elif elem['case'] == 'only_child_leaf':\n",
    "        elem['grandparent_def'] = wn.synset(elem['grandparents']).definition()\n",
    "        elem['parent_def'] = wn.synset(elem['parents']).definition()\n",
    "    elif elem['case'] == 'simple_triplet_2parent':\n",
    "        elem['1parent_def'] = wn.synset(elem['parents'][0]).definition()\n",
    "        elem['2parent_def'] = wn.synset(elem['parents'][1]).definition()\n",
    "    else:\n",
    "        elem['parent_def'] = wn.synset(elem['parents']).definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i, elem in enumerate(train):\n",
    "    try:\n",
    "        add_definitions(elem)\n",
    "    except:\n",
    "        print(i, elem)\n",
    "        train.remove(elem)\n",
    "\n",
    "counter = 0\n",
    "for i, elem in enumerate(test):\n",
    "    try:\n",
    "        add_definitions(elem)\n",
    "    except:\n",
    "        print(i, elem)\n",
    "        counter += 1\n",
    "        test.remove(elem)\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out = '/home/LLM_Taxonomy/wnet/unified_wnet_noun_verb_def_train.pickle'\n",
    "test_out = '/home/LLM_Taxonomy/wnet/unified_wnet_noun_verb_def_test.pickle'\n",
    "\n",
    "with open(train_out, 'wb') as f:\n",
    "    pickle.dump(train, f)\n",
    "\n",
    "with open(test_out, 'wb') as f:\n",
    "    pickle.dump(test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
