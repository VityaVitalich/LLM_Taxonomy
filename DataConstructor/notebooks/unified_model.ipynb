{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from leafer import Leafer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_path = '../../TaxonomyEnrichment/data/noun/train.edgelist'\n",
    "verb_path = '../../TaxonomyEnrichment/data/verb/train.edgelist'\n",
    "\n",
    "noun = nx.read_edgelist(noun_path, create_using=nx.DiGraph, delimiter='\\t')\n",
    "verb = nx.read_edgelist(verb_path, create_using=nx.DiGraph, delimiter='\\t')\n",
    "\n",
    "G = nx.compose(noun, verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude test\n",
    "\n",
    "# from MAGS\n",
    "cs_test_path = '../../TaxonomyEnrichment/data/MAG_CS/test_nodes.pickle'\n",
    "psy_test_path = '../../TaxonomyEnrichment/data/psychology/test_nodes.pickle'\n",
    "\n",
    "with open(cs_test_path, 'rb') as f:\n",
    "    cs_test = pickle.load(f)\n",
    "\n",
    "with open(psy_test_path, 'rb') as f:\n",
    "    psy_test = pickle.load(f)\n",
    "\n",
    "k = 0\n",
    "for node, parents in cs_test:\n",
    "    if node in G.nodes():\n",
    "        G.remove_node(node + '.n.01')\n",
    "        k += 1\n",
    "    \n",
    "for node, parents in psy_test:\n",
    "    if node in G.nodes():\n",
    "        G.remove_node(node + '.n.01')\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sertraline overdose'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entity.n.01',\n",
       " 'abstraction.n.06',\n",
       " 'physical_entity.n.01',\n",
       " 'thing.n.08',\n",
       " 'attribute.n.02']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(G.nodes())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NON SELECTIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_path = '../../TaxonomyEnrichment/data/noun/all.edgelist'\n",
    "verb_path = '../../TaxonomyEnrichment/data/verb/all.edgelist'\n",
    "\n",
    "noun = nx.read_edgelist(noun_path, create_using=nx.DiGraph, delimiter='\\t')\n",
    "verb = nx.read_edgelist(verb_path, create_using=nx.DiGraph, delimiter='\\t')\n",
    "\n",
    "G = nx.compose(noun, verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Leafer(G)\n",
    "# iterator = l.leafs_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_hypernym 44772 44772\n",
      "predict_hypernym 49 49\n"
     ]
    }
   ],
   "source": [
    "train, test = l.split_train_test(\n",
    "    generation_depth=0,  # до какого уровня в топ. сортировке идти\n",
    "    p=0.001,  # вероятность что подходящий случай уйдет в тест\n",
    "    p_divide_leafs=0.5,\n",
    "    # вероятность что листья поделим пополам трейн-тест\n",
    "    # а не засунем целый случай в трейн или в тест\n",
    "    min_to_test_rate=0.5,\n",
    "    # минимальное количество доли вершин которых не было в\n",
    "    # трейне чтобы поделить пополам на трейн-тест\n",
    "    # то есть если 6\\10 вершин были трейне то значит все 10 в трейн\n",
    "    # если 5\\10 были в трейне, то значит оставшиеся можем кинуть в тест\n",
    "    weights=[0.00, 0.0, 0.0, 0.00, 0.00, 1.],\n",
    "    # веса в соответствии\n",
    "    # один ребенок, только листья, не только листья\n",
    "    # триплеты с 2 родителями, триплеты такие что мидл нода имеет\n",
    "    # 1 ребенка, предсказание родителя\n",
    "    #p_parent=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'children': 'bdellium.n.01',\n",
       "   'parents': 'gum_resin.n.01',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'},\n",
       "  {'children': 'thinning_shears.n.01',\n",
       "   'parents': 'shears.n.01',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'},\n",
       "  {'children': 'spotweld.v.01',\n",
       "   'parents': 'weld.v.01',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'}],\n",
       " [{'children': 'broad_beech_fern.n.01',\n",
       "   'parents': 'beech_fern.n.01',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'},\n",
       "  {'children': 'impreciseness.n.01',\n",
       "   'parents': 'inexactness.n.01',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'},\n",
       "  {'children': 'black_guillemot.n.01',\n",
       "   'parents': 'guillemot.n.01',\n",
       "   'grandparents': None,\n",
       "   'case': 'predict_hypernym'}])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:3], test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_definitions(elem):\n",
    "    if elem['case'] == 'predict_hypernym':\n",
    "        elem['child_def'] = wn.synset(elem['children'].replace(' ', '_')).definition()\n",
    "      #  elem['parent_def'] = wn.synset(elem['parents']).definition()\n",
    "    elif elem['case'] == 'predict_multiple_hypernyms':\n",
    "        elem['child_def'] = wn.synset(elem['children'].replace(' ', '_')).definition()\n",
    "\n",
    "    elif elem['case'] == 'simple_triplet_grandparent':\n",
    "        elem['child_def'] = wn.synset(elem['children'].replace(' ', '_')).definition()\n",
    "        elem['grandparent_def'] = wn.synset(elem['grandparents']).definition()\n",
    "    elif elem['case'] == 'only_child_leaf':\n",
    "        elem['grandparent_def'] = wn.synset(elem['grandparents']).definition()\n",
    "        elem['parent_def'] = wn.synset(elem['parents']).definition()\n",
    "    elif elem['case'] == 'simple_triplet_2parent':\n",
    "        elem['1parent_def'] = wn.synset(elem['parents'][0]).definition()\n",
    "        elem['2parent_def'] = wn.synset(elem['parents'][1]).definition()\n",
    "    else:\n",
    "        elem['parent_def'] = wn.synset(elem['parents']).definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i, elem in enumerate(train):\n",
    "    try:\n",
    "        add_definitions(elem)\n",
    "    except:\n",
    "        print(i, elem)\n",
    "        train.remove(elem)\n",
    "\n",
    "counter = 0\n",
    "for i, elem in enumerate(test):\n",
    "    try:\n",
    "        add_definitions(elem)\n",
    "    except:\n",
    "        print(i, elem)\n",
    "        counter += 1\n",
    "        test.remove(elem)\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out = '/home/LLM_Taxonomy/wnet/unified_wnet_noun_verb_def_train.pickle'\n",
    "test_out = '/home/LLM_Taxonomy/wnet/unified_wnet_noun_verb_def_test.pickle'\n",
    "\n",
    "with open(train_out, 'wb') as f:\n",
    "    pickle.dump(train, f)\n",
    "\n",
    "with open(test_out, 'wb') as f:\n",
    "    pickle.dump(test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
