{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from leafer import Leafer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "for synset in (wn.all_synsets('n')):\n",
    "    name = synset.name()\n",
    "    G.add_node(name)\n",
    "    hyponyms = synset.hyponyms()\n",
    "\n",
    "    for hypo in hyponyms:\n",
    "        new_name = hypo.name()\n",
    "        G.add_node(new_name)\n",
    "        G.add_edge(name, new_name)\n",
    "\n",
    "for synset in (wn.all_synsets('v')):\n",
    "    name = synset.name()\n",
    "    G.add_node(name)\n",
    "    hyponyms = synset.hyponyms()\n",
    "\n",
    "    for hypo in hyponyms:\n",
    "        new_name = hypo.name()\n",
    "        G.add_node(new_name)\n",
    "        G.add_edge(name, new_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('restrain.v.01', 'inhibit.v.04'), ('inhibit.v.04', 'restrain.v.01')]\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        cycle = nx.find_cycle(G)\n",
    "        print(cycle)\n",
    "        G.remove_edge(*cycle[0])\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Leafer(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_hypernym 43850 43850\n",
      "leafs_and_no_leafs 4792 4792\n",
      "simple_triplet_2parent 1367 1367\n",
      "leafs_and_no_leafs 204 204\n",
      "predict_hypernym 828 828\n",
      "simple_triplet_2parent 170 170\n"
     ]
    }
   ],
   "source": [
    "p_case = {\n",
    "    'only_leafs': 0,\n",
    "    'only_leafs_divided': 0,\n",
    "    'not_only_leafs': 0.05,\n",
    "    'parents': 0.00001,\n",
    "    'simple_triplet_2parent': 0.1,\n",
    "    'simple_triplet_grandparent': 0,\n",
    "    'only_child_leaf': 0\n",
    "}\n",
    "\n",
    "train, test = l.split_train_test(\n",
    "    generation_depth=0,  # до какого уровня в топ. сортировке идти\n",
    "    p=p_case,  # вероятность что подходящий случай уйдет в тест\n",
    "    p_divide_leafs=0,\n",
    "    # вероятность что листья поделим пополам трейн-тест\n",
    "    # а не засунем целый случай в трейн или в тест\n",
    "    min_to_test_rate=1,\n",
    "    # минимальное количество доли вершин которых не было в\n",
    "    # трейне чтобы поделить пополам на трейн-тест\n",
    "    # то есть если 6\\10 вершин были трейне то значит все 10 в трейн\n",
    "    # если 5\\10 были в трейне, то значит оставшиеся можем кинуть в тест\n",
    "    weights=[0.00, 0.0, 0.1, 0.1, 0.00, 0.8],\n",
    "   # weights=[0.00, 0.0, 0., 0.0, 0.00, 1.],\n",
    "\n",
    "    # веса в соответствии\n",
    "    # один ребенок, только листья, не только листья\n",
    "    # триплеты с 2 родителями, триплеты такие что мидл нода имеет\n",
    "    # 1 ребенка, предсказание родителя\n",
    "    #p_parent=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_definitions(elem):\n",
    "    if elem['case'] == 'predict_hypernym':\n",
    "        elem['child_def'] = wn.synset(elem['children'].replace(' ', '_')).definition()\n",
    "      #  elem['parent_def'] = wn.synset(elem['parents']).definition()\n",
    "    elif elem['case'] == 'predict_multiple_hypernyms':\n",
    "        elem['child_def'] = wn.synset(elem['children'].replace(' ', '_')).definition()\n",
    "\n",
    "    elif elem['case'] == 'simple_triplet_grandparent':\n",
    "        elem['child_def'] = wn.synset(elem['children'].replace(' ', '_')).definition()\n",
    "        elem['grandparent_def'] = wn.synset(elem['grandparents']).definition()\n",
    "    elif elem['case'] == 'only_child_leaf':\n",
    "        elem['grandparent_def'] = wn.synset(elem['grandparents']).definition()\n",
    "        elem['parent_def'] = wn.synset(elem['parents']).definition()\n",
    "    elif elem['case'] == 'simple_triplet_2parent':\n",
    "        elem['1parent_def'] = wn.synset(elem['parents'][0]).definition()\n",
    "        elem['2parent_def'] = wn.synset(elem['parents'][1]).definition()\n",
    "    else:\n",
    "        elem['parent_def'] = wn.synset(elem['parents']).definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(3):\n",
    "    for i, elem in enumerate(train):\n",
    "        try:\n",
    "            add_definitions(elem)\n",
    "        except:\n",
    "            print(k, i, elem)\n",
    "            train.remove(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for k in range(3):\n",
    "\n",
    "    for i, elem in enumerate(test):\n",
    "        try:\n",
    "            add_definitions(elem)\n",
    "        except:\n",
    "            print(k, i, elem)\n",
    "            counter += 1\n",
    "            test.remove(elem)\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out = '/home/data/v.moskvoretskii/LLM_Taxonomy/wnet/taxollama3_train.pickle'\n",
    "test_out = '/home/data/v.moskvoretskii/LLM_Taxonomy/taxollama3_test.pickle'\n",
    "\n",
    "with open(train_out, 'wb') as f:\n",
    "    pickle.dump(train, f)\n",
    "\n",
    "with open(test_out, 'wb') as f:\n",
    "    pickle.dump(test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
