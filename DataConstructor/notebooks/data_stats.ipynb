{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../NLP-DL-Project-hypo-to-hyper/pipeline_src/\")\n",
    "\n",
    "from config.config import TaskConfig\n",
    "from dataset.dataset import init_data, HypernymDataset\n",
    "\n",
    "from dataset.prompt_schemas import (\n",
    "    hypo_term_hyper,\n",
    "    predict_child_from_2_parents,\n",
    "    predict_child_from_parent,\n",
    "    predict_child_with_parent_and_grandparent,\n",
    "    predict_children_with_parent_and_brothers,\n",
    "    predict_parent_from_child_granparent,\n",
    ")\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"babel_datasets/test_en_babel.pickle\"\n",
    "\n",
    "df = pd.read_pickle(test_path)\n",
    "\n",
    "transforms = {\n",
    "            \"only_child_leaf\": predict_parent_from_child_granparent,\n",
    "            \"only_leafs_all\": predict_child_from_parent,\n",
    "            \"only_leafs_divided\": predict_children_with_parent_and_brothers,\n",
    "            \"leafs_and_no_leafs\": predict_child_from_parent,\n",
    "            \"simple_triplet_grandparent\": predict_parent_from_child_granparent,\n",
    "            \"simple_triplet_2parent\": predict_child_from_2_parents,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(df):\n",
    "    uniq = set()\n",
    "    cases = {\"only_child_leaf\": 0,\n",
    "                \"only_leafs_all\": 0,\n",
    "                \"only_leafs_divided\": 0,\n",
    "                \"leafs_and_no_leafs\": 0,\n",
    "                \"simple_triplet_grandparent\": 0,\n",
    "                \"simple_triplet_2parent\": 0\n",
    "            }\n",
    "    for item in df:\n",
    "        cases[item['case']] += 1\n",
    "        for k in ['children', \"parents\", \"grandparents\"]:\n",
    "            if isinstance(item[k], list):\n",
    "                for i in item[k]:\n",
    "                    uniq.add(i)\n",
    "            else:\n",
    "                if item[k]:\n",
    "                    uniq.add(item[k])\n",
    "\n",
    "    return uniq, cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language:  en\n",
      "Train uniques:  61036\n",
      "Train Stats: \n",
      "{'only_child_leaf': 5337, 'only_leafs_all': 2707, 'only_leafs_divided': 2534, 'leafs_and_no_leafs': 4307, 'simple_triplet_grandparent': 6055, 'simple_triplet_2parent': 3500}\n",
      "Test uniques:  3264\n",
      "Test Stats: \n",
      "{'only_child_leaf': 304, 'only_leafs_all': 129, 'only_leafs_divided': 135, 'leafs_and_no_leafs': 190, 'simple_triplet_grandparent': 331, 'simple_triplet_2parent': 122}\n",
      "====================\n",
      "Language:  de\n",
      "Train uniques:  25686\n",
      "Train Stats: \n",
      "{'only_child_leaf': 2254, 'only_leafs_all': 1184, 'only_leafs_divided': 1137, 'leafs_and_no_leafs': 1427, 'simple_triplet_grandparent': 1895, 'simple_triplet_2parent': 3385}\n",
      "Test uniques:  1427\n",
      "Test Stats: \n",
      "{'only_child_leaf': 118, 'only_leafs_all': 52, 'only_leafs_divided': 58, 'leafs_and_no_leafs': 67, 'simple_triplet_grandparent': 93, 'simple_triplet_2parent': 134}\n",
      "====================\n",
      "Language:  es\n",
      "Train uniques:  32129\n",
      "Train Stats: \n",
      "{'only_child_leaf': 2926, 'only_leafs_all': 1324, 'only_leafs_divided': 1190, 'leafs_and_no_leafs': 2112, 'simple_triplet_grandparent': 2750, 'simple_triplet_2parent': 9885}\n",
      "Test uniques:  1648\n",
      "Test Stats: \n",
      "{'only_child_leaf': 171, 'only_leafs_all': 51, 'only_leafs_divided': 77, 'leafs_and_no_leafs': 69, 'simple_triplet_grandparent': 157, 'simple_triplet_2parent': 246}\n",
      "====================\n",
      "Language:  fr\n",
      "Train uniques:  31092\n",
      "Train Stats: \n",
      "{'only_child_leaf': 2882, 'only_leafs_all': 1294, 'only_leafs_divided': 1200, 'leafs_and_no_leafs': 1955, 'simple_triplet_grandparent': 2684, 'simple_triplet_2parent': 9420}\n",
      "Test uniques:  1515\n",
      "Test Stats: \n",
      "{'only_child_leaf': 133, 'only_leafs_all': 46, 'only_leafs_divided': 67, 'leafs_and_no_leafs': 71, 'simple_triplet_grandparent': 132, 'simple_triplet_2parent': 207}\n",
      "====================\n",
      "Language:  it\n",
      "Train uniques:  30530\n",
      "Train Stats: \n",
      "{'only_child_leaf': 2759, 'only_leafs_all': 1336, 'only_leafs_divided': 1161, 'leafs_and_no_leafs': 1912, 'simple_triplet_grandparent': 2499, 'simple_triplet_2parent': 7379}\n",
      "Test uniques:  1576\n",
      "Test Stats: \n",
      "{'only_child_leaf': 157, 'only_leafs_all': 66, 'only_leafs_divided': 68, 'leafs_and_no_leafs': 68, 'simple_triplet_grandparent': 140, 'simple_triplet_2parent': 170}\n",
      "====================\n",
      "Language:  ru\n",
      "Train uniques:  23955\n",
      "Train Stats: \n",
      "{'only_child_leaf': 2205, 'only_leafs_all': 1146, 'only_leafs_divided': 1032, 'leafs_and_no_leafs': 1337, 'simple_triplet_grandparent': 1837, 'simple_triplet_2parent': 2867}\n",
      "Test uniques:  1384\n",
      "Test Stats: \n",
      "{'only_child_leaf': 111, 'only_leafs_all': 56, 'only_leafs_divided': 50, 'leafs_and_no_leafs': 70, 'simple_triplet_grandparent': 91, 'simple_triplet_2parent': 136}\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for language in [\"en\", \"de\", \"es\", \"fr\", \"it\", \"ru\"]:\n",
    "    print(\"Language: \", language)\n",
    "    test_path = f\"babel_datasets/test_{language}_babel.pickle\"\n",
    "    train_path = f\"babel_datasets/train_{language}_babel.pickle\"\n",
    "\n",
    "    train_df = pd.read_pickle(train_path)\n",
    "    test_df = pd.read_pickle(test_path)\n",
    "\n",
    "    train_uniq, train_cases = get_stats(train_df)\n",
    "    test_uniq, test_cases = get_stats(test_df)\n",
    "\n",
    "    print(\"Train uniques: \", len(train_uniq))\n",
    "    print(\"Train Stats: \")\n",
    "    print(train_cases)\n",
    "\n",
    "    print(\"Test uniques: \", len(test_uniq))\n",
    "    print(\"Test Stats: \")\n",
    "    print(test_cases)\n",
    "    \n",
    "    print('====================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language:  en\n",
      "Train uniques:  39925\n",
      "Train Stats: \n",
      "{'only_child_leaf': 4093, 'only_leafs_all': 1616, 'only_leafs_divided': 2306, 'leafs_and_no_leafs': 2646, 'simple_triplet_grandparent': 3533, 'simple_triplet_2parent': 1343}\n",
      "Test uniques:  2296\n",
      "Test Stats: \n",
      "{'only_child_leaf': 213, 'only_leafs_all': 75, 'only_leafs_divided': 114, 'leafs_and_no_leafs': 129, 'simple_triplet_grandparent': 193, 'simple_triplet_2parent': 72}\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for language in [\"en\"]:\n",
    "    print(\"Language: \", language)\n",
    "    test_path = f\"babel_datasets/wnet_test_{language}_babel.pickle\"\n",
    "    train_path = f\"babel_datasets/wnet_train_{language}_babel.pickle\"\n",
    "\n",
    "    train_df = pd.read_pickle(train_path)\n",
    "    test_df = pd.read_pickle(test_path)\n",
    "\n",
    "    train_uniq, train_cases = get_stats(train_df)\n",
    "    test_uniq, test_cases = get_stats(test_df)\n",
    "\n",
    "    print(\"Train uniques: \", len(train_uniq))\n",
    "    print(\"Train Stats: \")\n",
    "    print(train_cases)\n",
    "\n",
    "    print(\"Test uniques: \", len(test_uniq))\n",
    "    print(\"Test Stats: \")\n",
    "    print(test_cases)\n",
    "    \n",
    "    print('====================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
