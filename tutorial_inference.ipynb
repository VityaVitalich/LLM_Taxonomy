{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "GPU\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3,4\"\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import wandb\n",
    "\n",
    "sys.path.append(\"../NLP-DL-Project-hypo-to-hyper/pipeline_src/\")\n",
    "\n",
    "\n",
    "from config.config import TaskConfig\n",
    "from train import CustomScheduler, train\n",
    "from logger.logger import WanDBWriter\n",
    "from trainer.train_epoch import train_epoch, predict\n",
    "from metrics.metrics import get_all_metrics\n",
    "from dataset.dataset import init_data\n",
    "from logger.logger import WanDBWriter\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"GPU\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"CPU\")\n",
    "\n",
    "\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /srv/home/rabikov/taxonomy_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /srv/home/rabikov/taxonomy_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/home/rabikov/taxonomy_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('FILE')}\n",
      "  warn(msg)\n",
      "/srv/home/rabikov/taxonomy_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/srv/home/rabikov/taxonomy_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/srv/home/rabikov/taxonomy_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/srv/home/rabikov/taxonomy_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n",
      "/srv/home/rabikov/taxonomy_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    LlamaForCausalLM,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, get_peft_model_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TaskConfig()\n",
    "\n",
    "config.batch_size = 8\n",
    "\n",
    "\n",
    "config.data_path = 'babel_datasets/wnet_train_en_babel.pickle'\n",
    "config.gold_path = (\n",
    "    None  # \"SemEval2018-Task9/training/gold/1A.english.training.gold.txt\"\n",
    ")\n",
    "config.test_data_path = 'babel_datasets/wnet_test_en_babel.pickle'\n",
    "config.test_gold_path = (\n",
    "    None  # \"SemEval2018-Task9/test/gold/1A.english.test.gold.txt\"\n",
    ")\n",
    "\n",
    "config.device = device\n",
    "config.using_peft = True\n",
    "config.model_type = \"Auto\"  # Auto or Llama\n",
    "config.wandb_log_dir = \"/raid/rabikov/wandb/\"\n",
    "config.model_checkpoint = \"EleutherAI/gpt-neo-1.3B\"\n",
    "config.exp_name = config.model_checkpoint.replace(\"/\", \"-\")\n",
    "config.saving_path = \"/raid/rabikov/model_checkpoints/\" + config.exp_name\n",
    "\n",
    "load_path = (\n",
    "    \"/raid/rabikov/model_checkpoints/\"\n",
    "    + \"EleutherAI-gpt-neo-1.3Breweighted_wnet_remove_all_from_labels_custom_multilang_epoch=0_MAP=0.09087864718127875.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.model_type == \"Auto\":\n",
    "    model_type = AutoModelForCausalLM\n",
    "    tokenizer_type = AutoTokenizer\n",
    "elif config.model_type == \"Llama\":\n",
    "    model_type = LlamaForCausalLM\n",
    "    tokenizer_type = LlamaTokenizer\n",
    "\n",
    "model = model_type.from_pretrained(\n",
    "    config.model_checkpoint,\n",
    "    # load_in_8bit=True,\n",
    "   # torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = tokenizer_type.from_pretrained(\n",
    "    config.model_checkpoint,\n",
    "    padding_side=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1572864 || all params: 1317148672 || trainable%: 0.11941431012580485\n"
     ]
    }
   ],
   "source": [
    "if config.using_peft:\n",
    "    LORA_R = 8\n",
    "    LORA_ALPHA = 16\n",
    "    LORA_DROPOUT = 0.05\n",
    "    LORA_TARGET_MODULES = [\n",
    "        \"q\",\n",
    "        \"v\",\n",
    "    ]\n",
    "\n",
    "    # model = prepare_model_for_int8_training(model)\n",
    "    config_lora = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        # target_modules=LORA_TARGET_MODULES,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, config_lora)\n",
    "    model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, train_loader, val_loader = init_data(tokenizer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'children': ['ferrocerium.n.1', 'misch_metal.n.1'],\n",
       " 'parents': 'pyrophoric_alloy.n.1',\n",
       " 'grandparents': None,\n",
       " 'case': 'only_leafs_all'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.data[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> Predict hyponyms for the word 'digit.n.1'.  Answer:<s>zero.n.2, three.n.1, four.n.1, five.n.1, six.n.1, seven.n.1, eight.n.1, nine.n.1, binary_digit.n.1, decimal_digit.n.1, duodecimal_digit.n.1, hexadecimal_digit.n.1, octal_digit.n.1, significant_digit.n.1, one.n.1, two.n.1\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[1]['input_seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(load_path, map_location='cpu')\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "del checkpoint\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.gen_args = {\n",
    "    \"no_repeat_ngram_size\": 2,\n",
    "    \"max_new_tokens\": 32,\n",
    "    \"num_return_sequences\": 2,\n",
    "    \"num_beams\": 15,\n",
    "    \"early_stopping\": True,\n",
    "    \"num_beam_groups\": 5,\n",
    "    \"diversity_penalty\": 1.0,\n",
    "    \"temperature\": 0.9,\n",
    "}\n",
    "\n",
    "\n",
    "config.gen_args = {\n",
    "    \"no_repeat_ngram_size\": 2,\n",
    "    \"num_beams\": 5,\n",
    "    \"early_stopping\": True,\n",
    "    \"max_new_tokens\": 8,\n",
    "    \"temperature\": 0.95,\n",
    "}\n",
    "\n",
    "config.gen_args = {\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"do_sample\": True,\n",
    "    \"num_beams\": 1,\n",
    "    \"min_new_tokens\": 16 - 1,\n",
    "    \"max_new_tokens\": 16,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 20,\n",
    "    \"num_return_sequences\": 2\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds, all_labels = predict(model, tokenizer, val_loader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def split(ls, size):\n",
    "    res = []\n",
    "\n",
    "    for i in range(0, len(ls)-1, size):\n",
    "        res.append(ls[i:i+size])\n",
    "    return res\n",
    "\n",
    "def get_one_sample(batch, model, config):\n",
    "\n",
    "    \n",
    "    terms, att_mask_terms, targets, input_seqs, att_mask_input, labels = batch\n",
    "    output_tokens = model.generate(\n",
    "        inputs=terms.to(config.device),\n",
    "        attention_mask=att_mask_terms.to(config.device),\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **config.gen_args,\n",
    "    )\n",
    "    pred_tokens = output_tokens[:, terms.size()[1] :]\n",
    "    pred_str = tokenizer.batch_decode(pred_tokens.cpu(), skip_special_tokens=True)\n",
    "    gold_str = tokenizer.batch_decode(targets, skip_special_tokens=True)\n",
    "\n",
    "    if len(pred_str) > len(gold_str):\n",
    "        pred_str = split(pred_str, config.gen_args['num_return_sequences'])\n",
    "\n",
    "    return pred_str, gold_str\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, tokenizer, val_loader, config, epoch=\"\", ans_load_path=None):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_golds = []\n",
    "    for batch in tqdm(val_loader):\n",
    "        pred, gold = get_one_sample(batch, model, config)\n",
    "\n",
    "        all_preds.append(pred)\n",
    "        all_golds.append(gold)\n",
    "\n",
    "    return all_preds, all_golds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/99 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [03:14<00:00,  1.96s/it]\n"
     ]
    }
   ],
   "source": [
    "all_preds, all_labels = predict(model, tokenizer, val_loader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['claudication syndromen.2 | hypernomenclaturen.3',\n",
       "  'claudication pain of the feet and legs, claudicating pain of lower'],\n",
       " ['carpals, tarsal, flexor digitorum superficialis,',\n",
       "  'pectoralis, latissimus dorsi, pectoralis major,'],\n",
       " ['rhomboidalis, rhombicis, rhomboideus, rh',\n",
       "  'rheumatoid rhombus, pyriform, trapezoid,'],\n",
       " ['bastard, godson, godchild, godmother, godfather,',\n",
       "  'sister, godson, godchild, godmother, godwren,'],\n",
       " ['liver donor, blood donor, organ source, recipient, donor, donor card',\n",
       "  'free citizen, citizen, person, human being, free person, citizen-subject'],\n",
       " ['mulatto, half-caste, halfbreed, mongrel',\n",
       "  'commoner, non-Jew, noncomic, nonconformist,'],\n",
       " ['atheist, conservative, liberal, agnostic, atheist, fundamentalist, Marxist,',\n",
       "  'critic, nonconformist, antiwar, liberal, radical, diss'],\n",
       " ['bloodsucker, bullfighter, cocky, bulldog, jester,',\n",
       "  \"matador, bullfight, bullfighter's cape, roper, rodeo\"]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chinchilla palm, coconut palm, mangrove palm, orchid palm, palm civet, tree palm, tree leg, kapok palm',\n",
       " 'palmetto, bromeliads, palm tree, palmettos, palm wine, senna, tamarind, pali, cactus,',\n",
       " 'cocor, cocoanut palm, eucalyptus, eupatorium, eugenia, khalwa, kopi, t',\n",
       " 'bunya, banana palm, banana tree, banana, banana leaf, banana nut, chameleon palm, date palm, dendrobium, g',\n",
       " 'acacia, acer, agapanthus, ackee, acorn, acumin, acushla, akalai, akimba, all',\n",
       " 'baby palm, palm tree, palm leaf, palm wine, palm root, taro, kadamba, manioc, palm-nut, kap',\n",
       " 'mango palm, banana palm, coconut palm, finger palm, kopje palm, laurustinus, mangrove palm, pine palm, pineapple',\n",
       " 'grapefruit tree, water palm, kapok tree, banana tree, banyan tree, coconut palm, palm tree, pineapple palm, pineapple tree',\n",
       " 'tangela, champa, chamomile, palm nut, palm olein, tangela palm, chiromantra palm, b',\n",
       " 'bougainvillaea, hibiscus, lily of the valley, hollyhock, nana, water palm, fig tree, b',\n",
       " 'chilean, karst palm, rubber palm, palm tree, sago palm, coconut palm, cacti palm, banana palm, cashew',\n",
       " 'cocoan, kalancho, guayule tree, jaguar, pea palm, pecan, palo, peyote,',\n",
       " 'aloe palm, American palm, cactus palm, palm tree, palmetto, palo verde, panda palm, kapok, palm',\n",
       " 'gigantea, tangelo, breadfruit, bread, bread tree, bread palm, bread apple, bread berry, bread vine, bread flower,',\n",
       " 'lizard palm, monkey palm, tree palm, acacia palm, African palm, Amazonian palm, Asian palm, Bornean palm, Chinese palm,',\n",
       " 'American, African, Bambara, Chibcha, Indian palm, Mopane, Parry, Bamboo, Cassava, Chinese, Guar',\n",
       " 'dwarf palm, common palm, sugar palm, rubber palm, tree palm, mangrove palm, peafowl, salt-water palm, palm',\n",
       " 'mala, bromeliad palm, banyan, palm tree, pinnate palm, kite tree, acacia, bambara,',\n",
       " 'palm-nut, water palm, sugar palm, coconut palm, eucalyptus, mangrove, rubber tree, cactus, banana tree',\n",
       " 'noseleaf, bromeliad, acacia palm, date palm, coconut palm, eucalyptus palm, ebony palm, kap',\n",
       " 'anaconda, banana palm, banana trunk palm, breadfruit palm, kapok palm, lapp, lemon palm, coconut palm, eucaly',\n",
       " 'lancia, cedarwood, eucalyptus, gum tree, palm tree, pomegranate, pine, pinecone, orange,',\n",
       " 'mali, pecans, coconut palm, mangrove palm, coconut, cocoanut, coconut-palm, coconutnut, coconut tree,',\n",
       " 'civet, banana palm, kopan palm, nyame palm, palm tree, palm civet and palm tree palm, tangelos',\n",
       " 'anaconda, palm tree, palm fruit, palm leaf, pisang, pecan, cactus-tree, banana-tree branch, banana,',\n",
       " 'cocos nucifera, banana palm, coffee palm, coconut palm, Indian palm, palm wine, coconut tree, breadfruit palm, banana tree',\n",
       " 'coconut palm, banana palm, guava palm, mangrove palm, water palm, taro palm, coconut palm, pineapple palm, rubber palm',\n",
       " 'bark palm, coconut palm, coco palm, mangrove palm, sugar palm, tea palm, salt tree palm, sweet palm, coffee palm,',\n",
       " 'birate palm, coconut palm, banana palm, date palm, palm fruit tree, rubber plant, rubber tree, yam, palm tree, sago',\n",
       " 'cocoa palm, coffee palm, guava palm, rubber palm, banana tree palm, sugar palm, breadfruit palm, palm tree, coconut palm,',\n",
       " 'cassia, cinnamon palm, coconut palm, date palm, fruit palm, guinea palm, lebar palm, mopan palm, sago palm',\n",
       " 'apple of peace, American palm, Asian palm, banana palm tree, cocoanut tree, coconut palm, Chinese palm, palm tree (the banana tree),',\n",
       " 'palo santorum, santorini, satsumana, shawarma, soursop, cassava palm, cinnamon palm, c',\n",
       " 'bunce-tree palm, coco-nut palm, pajara palm, palm tree, palm-nut, water palm, watery palm,',\n",
       " 'American palm, Brazilian palm, Australian palm, African palm, European palm tree, Korean palm, palm tree of Malaysia, palmett, palm-nut, palm',\n",
       " 'panda palm, sugar palm, tree palm, cocoanut, mangrove palm, coconut palm, banana palm, breadfruit palm, hibisc',\n",
       " 'narcissus, jacaranda, palmyra palm, banana palm, banyan palm, chirimen, kalanchoe, k',\n",
       " 'apple palm, banana palm, guava palm, coconut palm, kapok palm, lemon-sandal palm, mopan palm, olive palm,',\n",
       " 'bamboo palm, jasmine palm, banyan tree, bambara palm, palm palm, kapok palm, coconut palm, guay',\n",
       " 'bunya, mantis, banana, cactus, palm tree, cattail, banana tree, palm, bougainvillea, cork',\n",
       " 'palm frond, palm tree, palm wine, palm-nut, palm oil, palm sugar, palm toddy, palm nut, palmol, palm',\n",
       " 'bunthu, kabocha, mohua, nipa, pomelo, kapok, lemon, tamarind,',\n",
       " 'American palm, Mexican palm, palm tree, palmettia, parrot palm, pomegranate palm, roseate palm trees, water palm, oil',\n",
       " 'anaconda, kariba, kawaki, pea palm, sessile palm, tejada, terebinthus, cec',\n",
       " 'almond tree, fig tree, olive tree, orange tree, tamarind tree, water palm, coconut palm, guava tree, bread palm, banana',\n",
       " 'gum, guava, lime, kola, papaya, pomelo, rubber, sugar, palm nut, kiwi, sweet tree,',\n",
       " 'bokra palm, tibetan palm, African palm, Indian palm, mangrove palm, sugar palm, American palm, Asian palm, c',\n",
       " 'mimosa, mangrove palm, sea palm, tree palm, manana, palm tree, palm civet, bactrian camel, man',\n",
       " 'lulu tree, banyan tree, hibiscus, creeper, palm, pea palm, banana tree, cactus, guinea',\n",
       " 'pitha palm, banyan, carob, coconut palm, guava tree, mango palm, mangrove palm, pocan, pap']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(ls, size):\n",
    "    res = []\n",
    "\n",
    "    for i in range(0, len(ls)-1, size):\n",
    "        res.append(ls[i:i+size])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ground-air warning radar, ground-water warning radar (GWWR), weather radar, sea radar, weather forecast radar, infrared radar, radar altimeter,',\n",
       "  'aerial radar, civil radar, flight-path radar, shipboard radar, marine radar, microwave radar, radar altimeter, radar astronomy, radio-direction'],\n",
       " ['coconut palm, coconut palm, pinnate palm, rubber palm, sugarcane palm, tea palm, tangerine palm, oil palm,',\n",
       "  'acacia palm, cachiro, ejipe, palm frond, palm tree, palmettos, palm leaf, palmetto, cerc']]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['base radar, long-range radar warning system, short- range radar system',\n",
       "  'palmyra palm, African palm tree, cactus palm (cactus),'],\n",
       " ['three-dimensional radar, Doppler radar',\n",
       "  'cabbage palm, cabbage palm, cabbage palm, coconut, corozo, fishtail palm, nipa palm, royal palm'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred, gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(label):\n",
    "    all_words = label.split(',')\n",
    "    new_words = []\n",
    "    for word in all_words:\n",
    "        new_words.append(word.strip().split('.')[0])\n",
    "    \n",
    "    return ', '.join(new_words)\n",
    "all_labels2 = list(map(transform, all_labels1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds2 = list(map(lambda x: x.replace('.', ''), all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = get_all_metrics(all_labels2, all_preds2, limit=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('black_locust, clammy_locust', 'chrysanthemum')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels2[17], all_preds2[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MRR': 0.0, 'MAP': 0.0, 'P@1': 0.0, 'P@3': 0.0, 'P@5': 0.0, 'P@15': 0.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "saving_path = config.saving_predictions_path + config.exp_name + \"_\"\n",
    "\n",
    "with open(saving_path, \"wb\") as fp:\n",
    "    pickle.dump(all_preds, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('person, actor, film director, cinematography, filmmaker, visual arts, person',\n",
       " 'thrower, baseball player, jock, person')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds[4], all_labels[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxonomy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
