{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3,4\"\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import wandb\n",
    "\n",
    "sys.path.append(\"../NLP-DL-Project-hypo-to-hyper/pipeline_src/\")\n",
    "\n",
    "\n",
    "from config.config import TaskConfig\n",
    "from train import CustomScheduler, train\n",
    "from logger.logger import WanDBWriter\n",
    "from trainer.train_epoch import train_epoch, predict\n",
    "from metrics.metrics import get_all_metrics\n",
    "from dataset.dataset import init_data\n",
    "from logger.logger import WanDBWriter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device='cuda'\n",
    "    print('GPU')\n",
    "else:\n",
    "    device='cpu'\n",
    "    print('CPU')\n",
    "    \n",
    "    \n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /srv/home/rabikov/taxonomy_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /srv/home/rabikov/taxonomy_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/home/rabikov/taxonomy_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('FILE')}\n",
      "  warn(msg)\n",
      "/srv/home/rabikov/taxonomy_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/srv/home/rabikov/taxonomy_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/srv/home/rabikov/taxonomy_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/srv/home/rabikov/taxonomy_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n",
      "/srv/home/rabikov/taxonomy_env/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    LlamaForCausalLM\n",
    "\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TaskConfig()\n",
    "\n",
    "config.batch_size = 32\n",
    "\n",
    "config.data_path = \"SemEval2018-Task9/training/data/1A.english.training.data.txt\"\n",
    "config.gold_path = \"SemEval2018-Task9/training/gold/1A.english.training.gold.txt\"\n",
    "config.test_data_path = \"SemEval2018-Task9/test/data/1A.english.test.data.txt\"\n",
    "config.test_gold_path = \"SemEval2018-Task9/test/gold/1A.english.test.gold.txt\"\n",
    "\n",
    "config.device = device\n",
    "config.using_peft = False\n",
    "config.model_type = 'Auto' #Auto or Llama\n",
    "config.wandb_log_dir = '/raid/rabikov/wandb/'\n",
    "config.model_checkpoint = 'EleutherAI/gpt-neo-125m'\n",
    "config.exp_name = config.model_checkpoint.replace('/', '-')\n",
    "config.saving_path = '/raid/rabikov/model_checkpoints/' + config.exp_name\n",
    "\n",
    "load_path = '/raid/rabikov/model_checkpoints/' + 'EleutherAI-gpt-neo-125m_epoch=1_MAP=0.06440927737634727.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.model_type == 'Auto':\n",
    "    model_type = AutoModelForCausalLM\n",
    "    tokenizer_type = AutoTokenizer\n",
    "elif config.model_type == \"Llama\":\n",
    "    model_type = LlamaForCausalLM\n",
    "    tokenizer_type = LlamaTokenizer\n",
    "\n",
    "model = model_type.from_pretrained(config.model_checkpoint,\n",
    "    #load_in_8bit=True,\n",
    "    #torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",)\n",
    "\n",
    "tokenizer = tokenizer_type.from_pretrained(\n",
    "    config.model_checkpoint,\n",
    "    padding_side=\"left\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.using_peft:\n",
    "    LORA_R = 8\n",
    "    LORA_ALPHA = 16\n",
    "    LORA_DROPOUT= 0.05\n",
    "    LORA_TARGET_MODULES = [\n",
    "        \"q\",\n",
    "        \"v\",\n",
    "    ]\n",
    "    \n",
    "    #model = prepare_model_for_int8_training(model)\n",
    "    config_lora = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "    # target_modules=LORA_TARGET_MODULES,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, config_lora)\n",
    "    model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, train_loader, val_loader = init_data(tokenizer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(load_path)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "del checkpoint\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.gen_args = {\"no_repeat_ngram_size\":2,\n",
    "                    \"max_new_tokens\":32, \n",
    "                    \"num_return_sequences\": 2,\n",
    "                    \"num_beams\": 15,\n",
    "                    \"early_stopping\":True, \n",
    "                    \"num_beam_groups\":5, \n",
    "                    \"diversity_penalty\":1.0,\n",
    "                    \"temperature\": 0.9}\n",
    "\n",
    "\n",
    "config.gen_args = {\n",
    "            \"no_repeat_ngram_size\":2,\n",
    "            \"num_beams\": 15,\n",
    "            \"early_stopping\": True,\n",
    "            \"max_new_tokens\": 16,\n",
    "            \"temperature\": 0.9\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval going: 100%|██████████| 46/46 [03:22<00:00,  4.40s/it]\n"
     ]
    }
   ],
   "source": [
    "if config.using_peft:\n",
    "    all_preds, all_labels = predict(model.model, tokenizer, val_loader, config)\n",
    "else:\n",
    "    all_preds, all_labels = predict(model, tokenizer, val_loader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = get_all_metrics(all_labels, all_preds, limit=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MRR': 0.11209239130434782,\n",
       " 'MAP': 0.040734578424795796,\n",
       " 'P@1': 0.11209239130434782,\n",
       " 'P@3': 0.04359148550724647,\n",
       " 'P@5': 0.03497509057971022,\n",
       " 'P@15': 0.03198060808454835}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MRR': 0.18615591397849462,\n",
       " 'MAP': 0.07167302604802604,\n",
       " 'P@1': 0.18615591397849462,\n",
       " 'P@3': 0.0750448028673832,\n",
       " 'P@5': 0.062141577060932075,\n",
       " 'P@15': 0.05833561040214272}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('person, actor, film director, cinematography, filmmaker, visual arts, person',\n",
       " 'thrower, baseball player, jock, person')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds[4], all_labels[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxonomy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
