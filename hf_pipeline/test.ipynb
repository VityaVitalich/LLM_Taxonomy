{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/rabikov/anaconda3/envs/conda_taxonomy/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,3,4\"\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import wandb\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sys.path.append(\"../pipeline_src/\")\n",
    "\n",
    "\n",
    "from config.config import TaskConfig\n",
    "from train import CustomScheduler, train\n",
    "from logger.logger import WanDBWriter\n",
    "from trainer.train_epoch import train_epoch, predict\n",
    "from dataset.dataset import init_data\n",
    "from logger.logger import WanDBWriter\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"GPU\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"CPU\")\n",
    "\n",
    "\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, get_peft_model_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TaskConfig()\n",
    "\n",
    "config.n_epochs = 2\n",
    "config.batch_size = 32\n",
    "config.lr = 3e-4\n",
    "config.min_lr = 3e-6\n",
    "\n",
    "config.validation = 1\n",
    "config.save_every = 1\n",
    "config.compute_metrics_every = 1\n",
    "\n",
    "config.data_path = '../babel_datasets/wnet_only/train_ru_babel.pickle'\n",
    "config.gold_path = (\n",
    "    None  # \"SemEval2018-Task9/training/gold/1A.english.training.gold.txt\"\n",
    ")\n",
    "config.test_data_path = '../babel_datasets/wnet_only/test_ru_babel.pickle'\n",
    "config.test_gold_path = (\n",
    "    None  # \"SemEval2018-Task9/test/gold/1A.english.test.gold.txt\"\n",
    ")\n",
    "\n",
    "config.device = device\n",
    "config.using_peft = True\n",
    "config.model_type = \"Auto\"  # Auto or Llama\n",
    "config.wandb_log_dir = \"/raid/rabikov/wandb/\"\n",
    "config.model_checkpoint = \"EleutherAI/gpt-neo-125m\"\n",
    "config.exp_name = config.model_checkpoint.replace(\"/\", \"-\") + '_test'\n",
    "config.saving_path = \"/raid/rabikov/model_checkpoints/\" + config.exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from dataset.prompt_schemas import (\n",
    "    hypo_term_hyper,\n",
    "    predict_child_from_2_parents,\n",
    "    predict_child_from_parent,\n",
    "    predict_child_with_parent_and_grandparent,\n",
    "    predict_children_with_parent_and_brothers,\n",
    "    predict_parent_from_child_granparent,\n",
    ")\n",
    "import pandas as pd\n",
    "from multiprocessing import cpu_count\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class HypernymDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        tokenizer,\n",
    "        tokenizer_encode_args={\"return_tensors\": \"pt\"},\n",
    "        semeval_format=False,\n",
    "        gold_path=None,\n",
    "        transforms={\n",
    "            \"only_child_leaf\": predict_parent_from_child_granparent,\n",
    "            \"only_leafs_all\": predict_child_from_parent,\n",
    "            \"only_leafs_divided\": predict_children_with_parent_and_brothers,\n",
    "            \"leafs_and_no_leafs\": predict_child_from_parent,\n",
    "            \"simple_triplet_grandparent\": predict_parent_from_child_granparent,\n",
    "            \"simple_triplet_2parent\": predict_child_from_2_parents,\n",
    "        },\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        # self.transforms = transforms\n",
    "        # сюда могут идти немного другие аргументы если допустим я использую Dolly а не T5\n",
    "        self.tokenizer_encode_args = tokenizer_encode_args\n",
    "        # в формате SemEval дебильные датасеты, мы их тут соединим\n",
    "        if semeval_format:\n",
    "            assert gold_path is not None\n",
    "            train_data_en_data = pd.read_csv(\n",
    "                data_path, header=None, sep=\"\\t\", names=[\"term\", \"relation\"]\n",
    "            )\n",
    "            train_gold_en_data = pd.read_csv(gold_path, header=None, names=[\"hypernym\"])\n",
    "\n",
    "            self.df = pd.concat([train_data_en_data, train_gold_en_data], axis=1)[\n",
    "                [\"term\", \"hypernym\"]\n",
    "            ]\n",
    "        # предположительно в нашем датасете уже все ок, но это опицональная часть\n",
    "        else:\n",
    "            # self.df = pd.read_csv(\n",
    "            #     data_path, header=None, sep=\"\\t\", names=[\"term\", \"hypernym\"]\n",
    "            # )\n",
    "\n",
    "            self.data = pd.read_pickle(data_path)\n",
    "\n",
    "        # self.df.index = list(range(len(self.df)))\n",
    "\n",
    "        self.case2transform = transforms\n",
    "\n",
    "    # в данном случае выход под LM модельку с маск токеном -100\n",
    "    def __getitem__(self, index):\n",
    "        # row = self.df.loc[index]\n",
    "        # term = row[\"term\"]\n",
    "        # target = \", \".join(row[\"hypernym\"].split(\"\\t\"))\n",
    "        elem = self.data[index]\n",
    "        case = elem[\"case\"]\n",
    "\n",
    "        # if not \"changed\" in elem.keys():\n",
    "        #     for field in [\"children\", \"parents\", \"grandparents\", \"brothers\"]:\n",
    "        #         if field in elem.keys():\n",
    "        #             elem[field] = HypernymDataset.delete_techniqal(elem[field])\n",
    "        #             elem[\"changed\"] = True\n",
    "\n",
    "        # заранее пишу более общо, чтобы мы могли разне процессинги пробовать, а в будущем рандомно выбирать и тд\n",
    "        # это типа мы подаем список трансформаций затравок\n",
    "        # processed_term = self.transforms[0](term)\n",
    "        processed_term, target = self.case2transform[case](elem)\n",
    "\n",
    "        # токенизируем\n",
    "        encoded_term = self.tokenizer.encode(\n",
    "            processed_term, **self.tokenizer_encode_args\n",
    "        )\n",
    "        encoded_target = self.tokenizer.encode(\n",
    "            target, add_special_tokens=False, **self.tokenizer_encode_args\n",
    "        )\n",
    "\n",
    "        input_seq = torch.concat([encoded_term, encoded_target], dim=1)\n",
    "        labels = input_seq.clone()\n",
    "        labels[0, : encoded_term.size()[1]] = -100\n",
    "\n",
    "        return {\n",
    "            \"encoded_term\": encoded_term.squeeze(),  # думаю потребуется при генерации, или для сек 2 сек\n",
    "            \"encoded_target\": encoded_target.squeeze(0),  # отдельно токены для таргета\n",
    "            \"input_seq\": input_seq.squeeze(),  # полное предложение без масок\n",
    "            \"labels\": labels.squeeze(),  # маскированный контекст\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    @staticmethod\n",
    "    def delete_techniqal(elem):\n",
    "        if isinstance(elem, str):\n",
    "            if \".n.\" in elem:\n",
    "                return elem.split(\".\")[0].replace(\"_\", \" \")\n",
    "            else:\n",
    "                return elem.replace(\"_\", \" \")\n",
    "\n",
    "        elif isinstance(elem, list):\n",
    "            new_words = []\n",
    "            for word in elem:\n",
    "                new_words.append(HypernymDataset.delete_techniqal(word))\n",
    "            return new_words\n",
    "\n",
    "    # ничего необычного, складываем, паддим\n",
    "\n",
    "\n",
    "class Collator:\n",
    "    def __init__(self, pad_token_id, eos_token_id, mask_token_id):\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.mask_token_id = mask_token_id\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        terms = []\n",
    "        targets = []\n",
    "        inputs = []\n",
    "        labels = []\n",
    "\n",
    "       # print(batch)\n",
    "        for elem in batch:\n",
    "            terms.append(elem[\"encoded_term\"].flip(dims=[0]))\n",
    "            targets.append(elem[\"encoded_target\"])\n",
    "            inputs.append(elem[\"input_seq\"])\n",
    "            labels.append(elem[\"labels\"])\n",
    "\n",
    "        terms = torch.nn.utils.rnn.pad_sequence(\n",
    "            terms, batch_first=True, padding_value=self.pad_token_id\n",
    "        ).flip(dims=[1])\n",
    "        targets = torch.nn.utils.rnn.pad_sequence(\n",
    "            targets, batch_first=True, padding_value=self.eos_token_id\n",
    "        )\n",
    "        inputs = torch.nn.utils.rnn.pad_sequence(\n",
    "            inputs, batch_first=True, padding_value=self.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(\n",
    "            labels, batch_first=True, padding_value=self.mask_token_id\n",
    "        )\n",
    "\n",
    "        att_mask_inputs = torch.zeros_like(inputs)\n",
    "        att_mask_inputs[inputs != self.pad_token_id] = 1\n",
    "\n",
    "        att_mask_terms = torch.zeros_like(terms)\n",
    "        att_mask_terms[terms != self.pad_token_id] = 1\n",
    "\n",
    "        # return {'terms': terms, \n",
    "        #         'att_mask_terms': att_mask_terms,\n",
    "        #         'targets': targets,\n",
    "        #         'input_ids': inputs,\n",
    "        #         'attention_mask': att_mask_inputs,\n",
    "        #         'labels': labels}\n",
    "\n",
    "        return {#'terms': terms, \n",
    "                #'att_mask_terms': att_mask_terms,\n",
    "                #'targets': targets,\n",
    "                'input_ids': inputs,\n",
    "                'attention_mask': att_mask_inputs,\n",
    "                'labels': labels}\n",
    "\n",
    "\n",
    "def init_data(tokenizer, config, mask_label_token=-100, semeval_format=False):\n",
    "    # data\n",
    "    train_dataset = HypernymDataset(\n",
    "        data_path=config.data_path,\n",
    "        tokenizer=tokenizer,\n",
    "        gold_path=config.gold_path,\n",
    "        semeval_format=semeval_format,\n",
    "    )\n",
    "    test_dataset = HypernymDataset(\n",
    "        data_path=config.test_data_path,\n",
    "        tokenizer=tokenizer,\n",
    "        gold_path=config.test_gold_path,\n",
    "        semeval_format=semeval_format,\n",
    "    )\n",
    "\n",
    "    num_workers = cpu_count()\n",
    "\n",
    "    collator = Collator(\n",
    "        tokenizer.eos_token_id, tokenizer.eos_token_id, mask_label_token\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        collate_fn=collator,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        collate_fn=collator,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_dataset, test_dataset, train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.model_checkpoint,\n",
    "    padding_side=\"left\",\n",
    ")\n",
    "train_dataset, test_dataset, train_loader, val_loader = init_data(tokenizer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import partial\n",
    "\n",
    "# func = model.forward\n",
    "# def superforward(*args, **kwargs):\n",
    "#     return func(*args, **kwargs)\n",
    "# model.extra_forward = model.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "collator = Collator(\n",
    "        tokenizer.eos_token_id, tokenizer.eos_token_id, -100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import (\n",
    "    PREFIX_CHECKPOINT_DIR,\n",
    "    BestRun,\n",
    "    EvalLoopOutput,\n",
    "    EvalPrediction,\n",
    "    FSDPOption,\n",
    "    HPSearchBackend,\n",
    "    HubStrategy,\n",
    "    IntervalStrategy,\n",
    "    PredictionOutput,\n",
    "    RemoveColumnsCollator,\n",
    "    ShardedDDPOption,\n",
    "    TrainerMemoryTracker,\n",
    "    TrainOutput,\n",
    "    default_compute_objective,\n",
    "    denumpify_detensorize,\n",
    "    enable_full_determinism,\n",
    "    find_executable_batch_size,\n",
    "    get_last_checkpoint,\n",
    "    has_length,\n",
    "    number_of_arguments,\n",
    "    seed_worker )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Returns the training [`~torch.utils.data.DataLoader`].\n",
    "\n",
    "        Will use no sampler if `train_dataset` does not implement `__len__`, a random sampler (adapted to distributed\n",
    "        training if necessary) otherwise.\n",
    "\n",
    "        Subclass and override this method if you want to inject some custom behavior.\n",
    "        \"\"\"\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "\n",
    "        train_dataset = self.train_dataset\n",
    "        data_collator = self.data_collator\n",
    "       # if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n",
    "           # train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n",
    "      #  else:\n",
    "       #     data_collator = self._get_collator_with_removed_columns(data_collator, description=\"training\")\n",
    "\n",
    "        dataloader_params = {\n",
    "            \"batch_size\": self._train_batch_size,\n",
    "            \"collate_fn\": data_collator,\n",
    "            \"num_workers\": self.args.dataloader_num_workers,\n",
    "            \"pin_memory\": self.args.dataloader_pin_memory,\n",
    "        }\n",
    "\n",
    "        if not isinstance(train_dataset, torch.utils.data.IterableDataset):\n",
    "            dataloader_params[\"sampler\"] = self._get_train_sampler()\n",
    "            dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
    "            dataloader_params[\"worker_init_fn\"] = seed_worker\n",
    "\n",
    "        return self.accelerator.prepare(DataLoader(train_dataset, **dataloader_params))\n",
    "\n",
    "    def get_eval_dataloader(self, eval_dataset = None):\n",
    "        \"\"\"\n",
    "        Returns the evaluation [`~torch.utils.data.DataLoader`].\n",
    "\n",
    "        Subclass and override this method if you want to inject some custom behavior.\n",
    "\n",
    "        Args:\n",
    "            eval_dataset (`torch.utils.data.Dataset`, *optional*):\n",
    "                If provided, will override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns not accepted\n",
    "                by the `model.forward()` method are automatically removed. It must implement `__len__`.\n",
    "        \"\"\"\n",
    "        if eval_dataset is None and self.eval_dataset is None:\n",
    "            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "        data_collator = self.data_collator\n",
    "\n",
    "        # if is_datasets_available() and isinstance(eval_dataset, datasets.Dataset):\n",
    "        #     eval_dataset = self._remove_unused_columns(eval_dataset, description=\"evaluation\")\n",
    "        # else:\n",
    "        #     data_collator = self._get_collator_with_removed_columns(data_collator, description=\"evaluation\")\n",
    "\n",
    "        dataloader_params = {\n",
    "            \"batch_size\": self.args.eval_batch_size,\n",
    "            \"collate_fn\": data_collator,\n",
    "            \"num_workers\": self.args.dataloader_num_workers,\n",
    "            \"pin_memory\": self.args.dataloader_pin_memory,\n",
    "        }\n",
    "\n",
    "        if not isinstance(eval_dataset, torch.utils.data.IterableDataset):\n",
    "            dataloader_params[\"sampler\"] = self._get_eval_sampler(eval_dataset)\n",
    "            dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
    "\n",
    "        return self.accelerator.prepare(DataLoader(eval_dataset, **dataloader_params))\n",
    "\n",
    "    def get_test_dataloader(self, test_dataset):\n",
    "        \"\"\"\n",
    "        Returns the test [`~torch.utils.data.DataLoader`].\n",
    "\n",
    "        Subclass and override this method if you want to inject some custom behavior.\n",
    "\n",
    "        Args:\n",
    "            test_dataset (`torch.utils.data.Dataset`, *optional*):\n",
    "                The test dataset to use. If it is a [`~datasets.Dataset`], columns not accepted by the\n",
    "                `model.forward()` method are automatically removed. It must implement `__len__`.\n",
    "        \"\"\"\n",
    "        data_collator = self.data_collator\n",
    "\n",
    "        # if is_datasets_available() and isinstance(test_dataset, datasets.Dataset):\n",
    "        #     test_dataset = self._remove_unused_columns(test_dataset, description=\"test\")\n",
    "        # else:\n",
    "        #     data_collator = self._get_collator_with_removed_columns(data_collator, description=\"test\")\n",
    "\n",
    "        dataloader_params = {\n",
    "            \"batch_size\": self.args.eval_batch_size,\n",
    "            \"collate_fn\": data_collator,\n",
    "            \"num_workers\": self.args.dataloader_num_workers,\n",
    "            \"pin_memory\": self.args.dataloader_pin_memory,\n",
    "        }\n",
    "\n",
    "        if not isinstance(test_dataset, torch.utils.data.IterableDataset):\n",
    "            dataloader_params[\"sampler\"] = self._get_eval_sampler(test_dataset)\n",
    "            dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
    "\n",
    "        # We use the same batch_size as for eval.\n",
    "        return self.accelerator.prepare(DataLoader(test_dataset, **dataloader_params))\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'output_dir': 'tmp',\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"log_level\": \"error\",\n",
    "    # \"report_to\": \"wandb\",\n",
    "}\n",
    "\n",
    "train_args = {\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "   # 'gradient_accumulation_steps': 10,\n",
    "   # 'gradient_checkpointing': True,\n",
    "    # 'fp16': True,\n",
    "    \"optim\": \"adamw_torch\",\n",
    "    \"warmup_steps\": 200,\n",
    "    #\"max_steps\": 700,\n",
    "    'learning_rate': 3e-6,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"max_steps\": 200,\n",
    "    \"logging_steps\": 50,\n",
    "    \"disable_tqdm\": False,\n",
    "    \"save_strategy\": \"no\",\n",
    "    \"seed\": 0,\n",
    "}\n",
    "    \n",
    "training_args = TrainingArguments(**train_args, **default_args)\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    train_dataset=train_dataset, \n",
    "    args=training_args,\n",
    "    eval_dataset=test_dataset\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:37, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.860000</td>\n",
       "      <td>3.802648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.490200</td>\n",
       "      <td>3.710396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.483900</td>\n",
       "      <td>3.515179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.311900</td>\n",
       "      <td>3.331029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=3.5365215301513673, metrics={'train_runtime': 38.1748, 'train_samples_per_second': 10.478, 'train_steps_per_second': 5.239, 'total_flos': 66216973762560.0, 'train_loss': 3.5365215301513673, 'epoch': 0.22})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/13 00:00 < 00:01, 9.34 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.26 GiB (GPU 0; 10.92 GiB total capacity; 6.18 GiB already allocated; 3.75 GiB free; 6.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mpredict(test_dataset)\n",
      "File \u001b[0;32m/raid/rabikov/anaconda3/envs/conda_taxonomy/lib/python3.10/site-packages/transformers/trainer.py:3006\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3003\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   3005\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3006\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   3007\u001b[0m     test_dataloader, description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPrediction\u001b[39;49m\u001b[39m\"\u001b[39;49m, ignore_keys\u001b[39m=\u001b[39;49mignore_keys, metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix\n\u001b[1;32m   3008\u001b[0m )\n\u001b[1;32m   3009\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   3010\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/raid/rabikov/anaconda3/envs/conda_taxonomy/lib/python3.10/site-packages/transformers/trainer.py:3144\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3142\u001b[0m         logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_logits_for_metrics(logits, labels)\n\u001b[1;32m   3143\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mgather_for_metrics((logits))\n\u001b[0;32m-> 3144\u001b[0m     preds_host \u001b[39m=\u001b[39m logits \u001b[39mif\u001b[39;00m preds_host \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m nested_concat(preds_host, logits, padding_index\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m   3146\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3147\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mgather_for_metrics((labels))\n",
      "File \u001b[0;32m/raid/rabikov/anaconda3/envs/conda_taxonomy/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:116\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[39m=\u001b[39mpadding_index) \u001b[39mfor\u001b[39;00m t, n \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[39m=\u001b[39;49mpadding_index)\n\u001b[1;32m    117\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, Mapping):\n\u001b[1;32m    118\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensors)(\n\u001b[1;32m    119\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[39m=\u001b[39mpadding_index) \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m tensors\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    120\u001b[0m     )\n",
      "File \u001b[0;32m/raid/rabikov/anaconda3/envs/conda_taxonomy/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:81\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     78\u001b[0m new_shape \u001b[39m=\u001b[39m (tensor1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m tensor2\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mmax\u001b[39m(tensor1\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], tensor2\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])) \u001b[39m+\u001b[39m tensor1\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:]\n\u001b[1;32m     80\u001b[0m \u001b[39m# Now let's fill the result tensor\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m result \u001b[39m=\u001b[39m tensor1\u001b[39m.\u001b[39;49mnew_full(new_shape, padding_index)\n\u001b[1;32m     82\u001b[0m result[: tensor1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], : tensor1\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m tensor1\n\u001b[1;32m     83\u001b[0m result[tensor1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] :, : tensor2\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m tensor2\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.26 GiB (GPU 0; 10.92 GiB total capacity; 6.18 GiB already allocated; 3.75 GiB free; 6.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "res = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 7.6193, 'learning_rate': 7.5e-07, 'epoch': 0.05, 'step': 50},\n",
       " {'eval_loss': 7.019800186157227,\n",
       "  'eval_runtime': 1.9126,\n",
       "  'eval_samples_per_second': 53.852,\n",
       "  'eval_steps_per_second': 6.797,\n",
       "  'epoch': 0.05,\n",
       "  'step': 50},\n",
       " {'loss': 6.2721, 'learning_rate': 1.5e-06, 'epoch': 0.11, 'step': 100},\n",
       " {'eval_loss': 5.810854911804199,\n",
       "  'eval_runtime': 1.8679,\n",
       "  'eval_samples_per_second': 55.142,\n",
       "  'eval_steps_per_second': 6.96,\n",
       "  'epoch': 0.11,\n",
       "  'step': 100},\n",
       " {'loss': 5.3847, 'learning_rate': 2.25e-06, 'epoch': 0.16, 'step': 150},\n",
       " {'eval_loss': 4.6491899490356445,\n",
       "  'eval_runtime': 1.8718,\n",
       "  'eval_samples_per_second': 55.028,\n",
       "  'eval_steps_per_second': 6.945,\n",
       "  'epoch': 0.16,\n",
       "  'step': 150},\n",
       " {'loss': 4.528, 'learning_rate': 0.0, 'epoch': 0.22, 'step': 200},\n",
       " {'eval_loss': 3.9355380535125732,\n",
       "  'eval_runtime': 1.8556,\n",
       "  'eval_samples_per_second': 55.508,\n",
       "  'eval_steps_per_second': 7.006,\n",
       "  'epoch': 0.22,\n",
       "  'step': 200},\n",
       " {'train_runtime': 36.3804,\n",
       "  'train_samples_per_second': 10.995,\n",
       "  'train_steps_per_second': 5.497,\n",
       "  'total_flos': 66216973762560.0,\n",
       "  'train_loss': 5.951031723022461,\n",
       "  'epoch': 0.22,\n",
       "  'step': 200}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.state.log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.from_dict({'question': ['Predict hypernyms for word cat', 'Predict hyponyms for word dog'],\n",
    "                          'answer': ['animal', 'wild cat']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Predict hypernyms for word cat', 'answer': 'animal'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    }
   ],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"### Question: {example['question']}\\n ### Answer: {example['answer']}\"\n",
    "    return text\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['question'])):\n",
    "        text = f\"### Question: {example['question'][i]}\\n ### Answer: {example['answer'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    \"facebook/opt-350m\",\n",
    "    train_dataset=data,\n",
    "    #dataset_text_field=\"response\",\n",
    "    #packing=True,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    max_seq_length=512,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/rabikov/anaconda3/envs/conda_taxonomy/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/raid/rabikov/anaconda3/envs/conda_taxonomy/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=4.3858642578125, metrics={'train_runtime': 1.5037, 'train_samples_per_second': 3.99, 'train_steps_per_second': 1.995, 'total_flos': 174734180352.0, 'train_loss': 4.3858642578125, 'epoch': 3.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/rabikov/anaconda3/envs/conda_taxonomy/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5869371891021729,\n",
       " 'eval_runtime': 0.1766,\n",
       " 'eval_samples_per_second': 11.327,\n",
       " 'eval_steps_per_second': 5.664,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 41204], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_taxonomy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
